{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2c683da2-5fca-45ed-915c-add0bb5b6ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from utils import batch_generate, tokens_generate, run_inference\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import huggingface\n",
    "from datetime import datetime\n",
    "load_dotenv('secrets.env')\n",
    "\n",
    "device = 'mps'\n",
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "seed = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6f473f53-9814-4e39-9365-54dafa0d2a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/206747049/Desktop/mats/cot-validation/utils.py'>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5784cd89-cf0a-4b04-9448-4f63f6adb040",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/datasets/causal-nlp/CLadder/resolve/18d684e444ed07cc57ae0f3a91ee983a79b8119a/cladder.py",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/datasets/load.py:1647\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1647\u001b[0m     dataset_script_path \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _require_custom_configs \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:5218\u001b[0m, in \u001b[0;36mHfApi.hf_hub_download\u001b[0;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   5216\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 5218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5221\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5230\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5234\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:923\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:297\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    296\u001b[0m         next_url \u001b[38;5;241m=\u001b[39m urlparse(url)\u001b[38;5;241m.\u001b[39m_replace(path\u001b[38;5;241m=\u001b[39mparsed_target\u001b[38;5;241m.\u001b[39mpath)\u001b[38;5;241m.\u001b[39mgeturl()\n\u001b[0;32m--> 297\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:417\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    416\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67bab243-262d728874cddb9104f45373;4467f3ad-08ff-428f-b31b-9b77de2bd7fd)\n\nEntry Not Found for url: https://huggingface.co/datasets/causal-nlp/CLadder/resolve/18d684e444ed07cc57ae0f3a91ee983a79b8119a/cladder.py.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/datasets/causal-nlp/CLadder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcausalNLP/cladder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m cladder \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_v1.5_default\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshuffle(seed)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# r1-distill qwen 1.5b (tokenizer + model) \u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/datasets/load.py:1849\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1848\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1849\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/datasets/load.py:1696\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m         use_exported_dataset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1698\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is a gated dataset on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/datasets/load.py:1043\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mUSE_PARQUET_EXPORT \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_exported_dataset_infos:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         exported_dataset_infos \u001b[38;5;241m=\u001b[39m \u001b[43m_dataset_viewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exported_dataset_infos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m         exported_dataset_infos \u001b[38;5;241m=\u001b[39m DatasetInfosDict(\n\u001b[1;32m   1047\u001b[0m             {\n\u001b[1;32m   1048\u001b[0m                 config_name: DatasetInfo\u001b[38;5;241m.\u001b[39mfrom_dict(exported_dataset_infos[config_name])\n\u001b[1;32m   1049\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m config_name \u001b[38;5;129;01min\u001b[39;00m exported_dataset_infos\n\u001b[1;32m   1050\u001b[0m             }\n\u001b[1;32m   1051\u001b[0m         )\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m _dataset_viewer\u001b[38;5;241m.\u001b[39mDatasetViewerError:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/datasets/utils/_dataset_viewer.py:71\u001b[0m, in \u001b[0;36mget_exported_dataset_infos\u001b[0;34m(dataset, commit_hash, token)\u001b[0m\n\u001b[1;32m     69\u001b[0m dataset_viewer_info_url \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m://\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m://datasets-server.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/info?dataset=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     info_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_viewer_info_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_authentication_headers_for_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_ENDPOINT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     info_response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Revision\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info_response\u001b[38;5;241m.\u001b[39mheaders:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/urllib3/connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    706\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m     62\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:975\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# and socket type values to enum constants.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    976\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    977\u001b[0m     addrlist\u001b[38;5;241m.\u001b[39mappend((_intenum_converter(af, AddressFamily),\n\u001b[1;32m    978\u001b[0m                      _intenum_converter(socktype, SocketKind),\n\u001b[1;32m    979\u001b[0m                      proto, canonname, sa))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/causal-nlp/CLadder\n",
    "dataset = load_dataset('causalNLP/cladder')\n",
    "cladder = dataset['full_v1.5_default'].shuffle(seed)\n",
    "\n",
    "# r1-distill qwen 1.5b (tokenizer + model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# set seed \n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea557dc-0b4c-4c15-a0ec-735d5fdc4d39",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "Goal: test model performance on 100 cladder questions w/ chain of thought reasoning =D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fec77470-8343-4a62-b7de-fcbb9685f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "/Users/206747049/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2137: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on mps. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('mps') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m batches \u001b[38;5;241m=\u001b[39m batch_generate(cladder_df)\n\u001b[1;32m      4\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens_generate(batches, tokenizer \u001b[38;5;241m=\u001b[39m tokenizer)\n\u001b[0;32m----> 5\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_tracking\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/utils.py:66\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(model, tokens, time_tracking)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_tracking:\n\u001b[1;32m     64\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_tracking:\n\u001b[1;32m     69\u001b[0m         inference_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:819\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 819\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:533\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m DynamicCache()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "cladder_df = pd.DataFrame(cladder[0:100])\n",
    "\n",
    "batches = batch_generate(cladder_df)\n",
    "tokens = tokens_generate(batches, tokenizer = tokenizer)\n",
    "res = run_inference(model, tokens, time_tracking = True)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "85c12397-3a01-4056-8380-b38493ce4fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,\n",
       "              656,  95224,     11,  58059,   1879,    448,   1172,    279,   2701,\n",
       "             4682,     11,    323,   2041,    894,    650,  36591,   9363,    476,\n",
       "            58457,  11871,     25,  76834,   5269,   4714,    702,    264,   2118,\n",
       "             2456,    389,   8777,   2022,   2639,     13,  53583,  24854,    702,\n",
       "              264,   2118,   2456,    389,   8777,   2022,   2639,     13,    576,\n",
       "             8084,  18927,    315,  41531,   4714,    374,    220,     24,     18,\n",
       "            14360,   1752,   1251,    448,    902,  41531,   4714,    323,   2477,\n",
       "             2832,   6298,   1506,   7775,     11,    279,  18927,    315,  10865,\n",
       "            24854,    374,    220,     24,     24,  14360,   1752,   1251,    448,\n",
       "              902,  41531,   4714,    323,  69240,   7775,     11,    279,  18927,\n",
       "              315,  10865,  24854,    374,    220,     22,     19,  14360,   1752,\n",
       "             1251,    448,  41531,   4714,    323,   2477,   2832,   6298,   1506,\n",
       "             7775,     11,    279,  18927,    315,  10865,  24854,    374,    220,\n",
       "               24,     16,  14360,   1752,   1251,    448,  41531,   4714,    323,\n",
       "            69240,   7775,     11,    279,  18927,    315,  10865,  24854,    374,\n",
       "              220,     21,     24,  14360,   1416,    582,   1401,    518,  69240,\n",
       "             7775,     11,   1558,    279,   6012,    315,  10865,  24854,   5263,\n",
       "              979,  41531,   4714,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,    467,    579,     75,\n",
       "              702,    264,   2118,   2456,    389,    348,    392,     81,    323,\n",
       "             1147,    454,     71,     13,   1863,    454,     71,    702,    264,\n",
       "             2118,   2456,    389,  29663,   8345,     13,    647,    392,     81,\n",
       "              702,    264,   2118,   2456,    389,  29663,   8345,     13,   6730,\n",
       "              220,     16,     25,   1205,   1401,    518,   1246,   1147,    454,\n",
       "               71,  96203,    448,  29663,   8345,   1142,    553,   1142,   4092,\n",
       "              311,    348,    392,     81,     13,   6730,    220,     17,     25,\n",
       "             1205,   1401,   5961,    518,   1246,   1147,    454,     71,  96203,\n",
       "              448,  29663,   8345,    304,   4586,     13,   2014,   3535,   1246,\n",
       "             1147,    454,     71,  21501,  29663,   8345,     11,    374,    432,\n",
       "              803,   4396,    311,    990,    279,   6730,    220,     16,   1091,\n",
       "             6730,    220,     17,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151646,  51057,    264,    656,  95224,     11,\n",
       "            58059,   1879,    448,   1172,    279,   2701,   4682,     11,    323,\n",
       "             2041,    894,    650,  36591,   9363,    476,  58457,  11871,     25,\n",
       "              431,    941,     80,    702,    264,   2118,   2456,    389,   1147,\n",
       "              454,     71,    323,    856,   5120,     84,     13,   1863,    454,\n",
       "               71,    702,    264,   2118,   2456,    389,    856,   5120,     84,\n",
       "               13,    576,   8084,  18927,    315,    435,    941,     80,    374,\n",
       "              220,     18,     21,  14360,    576,  18927,    315,    537,    435,\n",
       "              941,     80,    323,    856,   5120,     84,    374,    220,     20,\n",
       "               18,  14360,    576,  18927,    315,    435,    941,     80,    323,\n",
       "              856,   5120,     84,    374,    220,     16,     21,  14360,   2160,\n",
       "              279,   6012,    315,    856,   5120,     84,   9155,    979,  44971,\n",
       "              435,    941,     80,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151646,  51057,    264,    656,  95224,     11,  58059,\n",
       "             1879,    448,   1172,    279,   2701,   4682,     11,    323,   2041,\n",
       "              894,    650,  36591,   9363,    476,  58457,  11871,     25,  73359,\n",
       "              702,    264,   2118,   2456,    389,   7403,    323,  16624,   8866,\n",
       "               13,  42408,    702,    264,   2118,   2456,    389,  16624,   8866,\n",
       "               13,   1752,  60854,    429,   1513,    944,    738,    279,  16624,\n",
       "               11,    279,  18927,    315,  67043,  16624,    374,    220,     20,\n",
       "               18,  14360,   1752,  60854,    429,    738,    279,  16624,     11,\n",
       "              279,  18927,    315,  67043,  16624,    374,    220,     23,     17,\n",
       "            14360,   4841,  16624,    738,    553,   9972,  18472,    279,   6012,\n",
       "              315,  67043,  16624,     30],\n",
       "          [151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,   1863,    454,     71,\n",
       "              702,    264,   2118,   2456,    389,    435,    941,     80,    323,\n",
       "              856,   5120,     84,     13,    431,    941,     80,    702,    264,\n",
       "             2118,   2456,    389,    856,   5120,     84,     13,   1752,   1846,\n",
       "              879,    525,    537,   1147,    454,     71,    323,    525,    537,\n",
       "              435,    941,     80,     11,    279,  18927,    315,    856,   5120,\n",
       "               84,    374,    220,     20,     24,  14360,   1752,   1846,    879,\n",
       "              525,    537,   1147,    454,     71,    323,    525,    435,    941,\n",
       "               80,     11,    279,  18927,    315,    856,   5120,     84,    374,\n",
       "              220,     19,     19,  14360,   1752,   1846,    879,    525,   1147,\n",
       "              454,     71,    323,    525,    537,    435,    941,     80,     11,\n",
       "              279,  18927,    315,    856,   5120,     84,    374,    220,     18,\n",
       "               19,  14360,   1752,   1846,    879,    525,   1147,    454,     71,\n",
       "              323,    525,    435,    941,     80,     11,    279,  18927,    315,\n",
       "              856,   5120,     84,    374,    220,     18,  14360,    576,   8084,\n",
       "            18927,    315,   1147,    454,     71,    374,    220,     24,     15,\n",
       "            14360,   4841,    435,    941,     80,   5263,    279,   6012,    315,\n",
       "              856,   5120,     84,     30]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['no', 'yes', 'yes', 'no', 'no'],\n",
       "  'metadata': {'id': [7455, 28610, 20926, 128, 20863],\n",
       "   'reasoning': ['Let Y = broken bones; X = respiratory issues; V3 = hospitalization status.\\nX->V3,Y->V3\\nP(Y = 1 | X = 1, V3 = 1] - P(Y = 1 | V3 = 1)\\nP(Y=1 | X=1, V3=1) - (P(X=1) * P(Y=1 | X=1, V3=1) + P(X=0) * P(Y=1 | X=0, V3=1))\\nP(X=1) = 0.93\\nP(Y=1 | X=0, V3=0) = 0.99\\nP(Y=1 | X=0, V3=1) = 0.74\\nP(Y=1 | X=1, V3=0) = 0.91\\nP(Y=1 | X=1, V3=1) = 0.69\\n0.69 - (0.93*0.69 + 0.07*0.74) = -0.00\\n0.00 = 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let X = rixq; V2 = zuph; Y = xevu.\\nX->V2,X->Y,V2->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.36\\nP(Y=1, X=0=1) = 0.53\\nP(Y=1, X=1=1) = 0.16\\n0.16/0.36 - 0.53/0.64 = -0.36\\n-0.36 < 0',\n",
       "    'Let X = husband; V2 = wife; Y = alarm clock.\\nX->V2,X->Y,V2->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.53\\nP(Y=1 | X=1) = 0.82\\n0.82 - 0.53 = 0.29\\n0.29 > 0',\n",
       "    'Let V1 = zuph; X = rixq; Y = xevu.\\nV1->X,V1->Y,X->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\n\\\\sum_{V1=v} P(V1=v)*[P(Y=1|V1=v,X=1) - P(Y=1|V1=v, X=0)]\\nP(Y=1 | V1=0, X=0) = 0.59\\nP(Y=1 | V1=0, X=1) = 0.44\\nP(Y=1 | V1=1, X=0) = 0.34\\nP(Y=1 | V1=1, X=1) = 0.03\\nP(V1=1) = 0.90\\n0.10 * (0.44 - 0.59) + 0.90 * (0.03 - 0.34) = -0.29\\n-0.29 < 0'],\n",
       "   'rung': [1, 2, 1, 2, 2],\n",
       "   'query_type': ['exp_away', 'backadj', 'correlation', 'ate', 'ate'],\n",
       "   'graph_id': ['collision',\n",
       "    'diamondcut',\n",
       "    'mediation',\n",
       "    'mediation',\n",
       "    'confounding'],\n",
       "   'story_id': ['hospitalization',\n",
       "    'nonsense7',\n",
       "    'nonsense0',\n",
       "    'alarm',\n",
       "    'nonsense0'],\n",
       "   'question_property': ['hard', 'nonsense', 'nonsense', 'easy', 'nonsense'],\n",
       "   'formal_form': ['P(Y = 1 | X = 1, V3 = 1] - P(Y = 1 | V3 = 1)',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    'P(Y | X)',\n",
       "    'E[Y | do(X = 1)] - E[Y | do(X = 0)]',\n",
       "    'E[Y | do(X = 1)] - E[Y | do(X = 0)]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,    220,     17,     30],\n",
       "          [151643, 151643, 151643,  ...,    454,     71,     30],\n",
       "          [151643, 151643, 151643,  ...,     88,   4554,     30],\n",
       "          [151643, 151643, 151643,  ...,    220,     17,     30],\n",
       "          [151646,  51057,    264,  ...,   9292,  76551,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'no', 'no', 'no'],\n",
       "  'metadata': {'id': [15901, 30911, 27608, 14319, 12704],\n",
       "   'reasoning': ['nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let V1 = jyka; X = zuph; Y = glimx.\\nV1->X,V1->Y,X->Y\\nY_{X=1} = 0 | V1=1\\nSolve for Y, given the evidence and the action\\nV1 = 1\\nX = V1\\nY = V1 or X\\nY = 1 = 1 or 1\\n0',\n",
       "    'Let V2 = qwiu; X = jyka; V3 = yupt; Y = kwox.\\nX->V3,V2->V3,X->Y,V2->Y,V3->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.46\\nP(Y=1 | X=1) = 0.61\\n0.61 - 0.46 = 0.15\\n0.15 > 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let V2 = residency status; X = gender; V3 = department competitiveness; Y = brown eyes.\\nX->V3,V2->V3,X->Y,V2->Y,V3->Y\\nE[Y_{X=0, V3=1} - Y_{X=0, V3=0}]\\n\\\\sum_{V3=v} [\\\\sum_{V2=k} P(Y=1|X=0,V3=v)*[P(V3=v|X=1,V2=k)-P(V3=v|X=0,V2=k)]*P(V2=k)]\\nP(Y=1 | X=0, V3=0) = 0.64\\nP(Y=1 | X=0, V3=1) = 0.39\\nP(Y=1 | X=1, V3=0) = 0.65\\nP(Y=1 | X=1, V3=1) = 0.39\\nP(V3=1 | X=0, V2=0) = 0.51\\nP(V3=1 | X=0, V2=1) = 0.21\\nP(V3=1 | X=1, V2=0) = 0.78\\nP(V3=1 | X=1, V2=1) = 0.47\\nP(V2=1) = 0.93\\n0.93 * 0.39 * (0.47 - 0.78)+ 0.07 * 0.39 * (0.21 - 0.51)= -0.06\\n-0.06 < 0'],\n",
       "   'rung': [2, 3, 3, 2, 3],\n",
       "   'query_type': ['backadj', 'det-counterfactual', 'ett', 'backadj', 'nie'],\n",
       "   'graph_id': ['fork',\n",
       "    'confounding',\n",
       "    'arrowhead',\n",
       "    'confounding',\n",
       "    'arrowhead'],\n",
       "   'story_id': ['forest_fire',\n",
       "    'nonsense9',\n",
       "    'nonsense5',\n",
       "    'simpson_vaccine',\n",
       "    'gender_admission_state'],\n",
       "   'question_property': ['anticommonsense',\n",
       "    'nonsense',\n",
       "    'nonsense',\n",
       "    'anticommonsense',\n",
       "    'anticommonsense'],\n",
       "   'formal_form': ['[backdoor adjustment set for Y given X]',\n",
       "    'Y_{X=1} = 0 | V1=1',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    'E[Y_{X=0, V3=1} - Y_{X=0, V3=0}]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,    454,     71,     30],\n",
       "          [151643, 151643, 151643,  ...,  44971,  19578,     30],\n",
       "          [151646,  51057,    264,  ...,     88,   4554,     30],\n",
       "          [151643, 151643, 151643,  ...,    279,  37799,     30],\n",
       "          [151643, 151643, 151643,  ...,    220,     17,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       "  'labels': ['yes', 'no', 'yes', 'yes', 'yes'],\n",
       "  'metadata': {'id': [29501, 7892, 23214, 265, 22376],\n",
       "   'reasoning': ['Let X = zuph; V3 = zory; V2 = jyka; Y = glimx.\\nX->V3,X->V2,V2->Y,V3->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.19\\nP(Y=1 | X=1) = 0.43\\n0.43 - 0.19 = 0.24\\n0.24 > 0',\n",
       "    'Let X = smoking; V2 = effort; Y = college admission.\\nX->V2,X->Y,V2->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.98\\nP(Y=1, X=0=1) = 0.01\\nP(Y=1, X=1=1) = 0.64\\n0.64/0.98 - 0.01/0.02 = -0.27\\n-0.27 < 0',\n",
       "    'Let V1 = qwiu; X = jyka; V3 = yupt; Y = kwox.\\nV1->X,X->V3,V1->Y,V3->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\n\\\\sum_{V3=v}P(Y=1|X=1,V3=v)*[P(V3=v|X=1)-P(V3=v|X=0)]\\nP(Y=1 | X=0, V3=0) = 0.62\\nP(Y=1 | X=0, V3=1) = 0.76\\nP(Y=1 | X=1, V3=0) = 0.61\\nP(Y=1 | X=1, V3=1) = 0.76\\nP(V3=1 | X=0) = 0.60\\nP(V3=1 | X=1) = 0.85\\n0.76 * (0.85 - 0.60) + 0.76 * (0.15 - 0.40) = 0.04\\n0.04 > 0',\n",
       "    'Let V2 = the candle; X = the man in the room; Y = room.\\nX->Y,V2->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.26\\nP(Y=1, X=0=1) = 0.41\\nP(Y=1, X=1=1) = 0.22\\n0.22/0.26 - 0.41/0.74 = 0.31\\n0.31 > 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan'],\n",
       "   'rung': [3, 1, 3, 1, 2],\n",
       "   'query_type': ['ett', 'correlation', 'ett', 'correlation', 'backadj'],\n",
       "   'graph_id': ['diamond', 'mediation', 'frontdoor', 'fork', 'arrowhead'],\n",
       "   'story_id': ['nonsense9',\n",
       "    'neg_mediation',\n",
       "    'nonsense5',\n",
       "    'candle',\n",
       "    'nonsense3'],\n",
       "   'question_property': ['nonsense', 'hard', 'nonsense', 'easy', 'nonsense'],\n",
       "   'formal_form': ['E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'P(Y | X)',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'P(Y | X)',\n",
       "    '[backdoor adjustment set for Y given X]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,    454,     71,     30],\n",
       "          [151643, 151643, 151643,  ...,    279,  24467,     30],\n",
       "          [151643, 151643, 151643,  ...,  16624,   8084,     30],\n",
       "          [151646,  51057,    264,  ...,    454,     71,     30],\n",
       "          [151643, 151643, 151643,  ...,   1126,     89,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'no', 'yes', 'yes'],\n",
       "  'metadata': {'id': [30994, 8718, 4814, 29345, 21629],\n",
       "   'reasoning': ['Let V1 = jyka; V3 = zory; X = zuph; Y = glimx.\\nV1->V3,V1->X,X->Y,V3->Y\\nY_{X=1} = 1 | V1=1\\nSolve for Y, given the evidence and the action\\nV1 = 1\\nX = V1\\nV3 = not X\\nY = X and V3\\nY = 0 = 1 and 0\\n0',\n",
       "    'Let V1 = pre-conditions; X = vaccination; Y = disease.\\nV1->X,V1->Y,X->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\n\\\\sum_{V1=v} P(V1=v|X=1)*[P(Y=1|V1=v,X=1) - P(Y=1|V1=v, X=0)]\\nP(Y=1 | V1=0, X=0) = 0.94\\nP(Y=1 | V1=0, X=1) = 0.74\\nP(Y=1 | V1=1, X=0) = 0.24\\nP(Y=1 | V1=1, X=1) = 0.03\\nP(V1=1) = 0.42\\n0.58 * (0.74 - 0.94) + 0.42 * (0.03 - 0.24) = -0.20\\n-0.20 < 0',\n",
       "    'Let X = husband; V2 = wife; Y = alarm clock.\\nX->V2,X->Y,V2->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.04\\nP(Y=1 | X=0) = 0.90\\nP(Y=1 | X=1) = 0.31\\n0.04*0.31 - 0.96*0.90 = 0.88\\n0.88 > 0',\n",
       "    'Let V1 = zory; X = zuph; V3 = jyka; Y = glimx.\\nV1->X,X->V3,V1->Y,V3->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\n\\\\sum_{V3=v}P(Y=1|X=1,V3=v)*[P(V3=v|X=1)-P(V3=v|X=0)]\\nP(Y=1 | X=0, V3=0) = 0.35\\nP(Y=1 | X=0, V3=1) = 0.54\\nP(Y=1 | X=1, V3=0) = 0.35\\nP(Y=1 | X=1, V3=1) = 0.54\\nP(V3=1 | X=0) = 0.08\\nP(V3=1 | X=1) = 0.24\\n0.54 * (0.24 - 0.08) + 0.54 * (0.76 - 0.92) = 0.03\\n0.03 > 0',\n",
       "    'Let V1 = swoq; V3 = muvy; X = kwox; Y = kwoz.\\nV1->V3,V1->X,X->Y,V3->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\n\\\\sum_{V1=v} P(V1=v)*[P(Y=1|V1=v,X=1) - P(Y=1|V1=v, X=0)]\\nP(Y=1 | V1=0, X=0) = 0.67\\nP(Y=1 | V1=0, X=1) = 0.10\\nP(Y=1 | V1=1, X=0) = 0.84\\nP(Y=1 | V1=1, X=1) = 0.14\\nP(V1=1) = 0.90\\n0.10 * (0.10 - 0.67) 0.90 * (0.14 - 0.84) = -0.68\\n-0.68 < 0'],\n",
       "   'rung': [3, 3, 1, 3, 2],\n",
       "   'query_type': ['det-counterfactual', 'ett', 'marginal', 'ett', 'ate'],\n",
       "   'graph_id': ['diamondcut',\n",
       "    'confounding',\n",
       "    'mediation',\n",
       "    'frontdoor',\n",
       "    'diamondcut'],\n",
       "   'story_id': ['nonsense9',\n",
       "    'simpson_vaccine',\n",
       "    'alarm',\n",
       "    'nonsense9',\n",
       "    'nonsense1'],\n",
       "   'question_property': ['nonsense', 'hard', 'hard', 'nonsense', 'nonsense'],\n",
       "   'formal_form': ['Y_{X=1} = 1 | V1=1',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'P(Y)',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'E[Y | do(X = 1)] - E[Y | do(X = 0)]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  60616,    702,    264,\n",
       "             2118,   2456,    389,  32551,     13,  54507,    702,    264,   2118,\n",
       "             2456,    389,  32551,     13,    576,   8084,  18927,    315,  18879,\n",
       "            11094,    374,    220,     23,     24,  14360,   1752,   1251,   6509,\n",
       "              650,    266,  69078,    323,    525,    537,  11245,     11,    279,\n",
       "            18927,    315,  10772,    374,    220,     24,     24,  14360,   1752,\n",
       "             1251,   6509,    650,    266,  69078,    323,    525,  11245,     11,\n",
       "              279,  18927,    315,  10772,    374,    220,     24,     22,  14360,\n",
       "             1752,   1251,   6509,  18879,    323,    525,    537,  11245,     11,\n",
       "              279,  18927,    315,  10772,    374,    220,     24,     24,  14360,\n",
       "             1752,   1251,   6509,  18879,    323,    525,  11245,     11,    279,\n",
       "            18927,    315,  10772,    374,    220,     20,     22,  14360,   1416,\n",
       "              582,   1401,    518,   1251,    879,    525,  11245,     11,   1558,\n",
       "              279,   6012,    315,  10772,  18472,    979,  18879,  11094,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,\n",
       "              656,  95224,     11,  58059,   1879,    448,   1172,    279,   2701,\n",
       "             4682,     11,    323,   2041,    894,    650,  36591,   9363,    476,\n",
       "            58457,  11871,     25,    809,   7564,    702,    264,   2118,   2456,\n",
       "              389,   2804,  16632,     84,    323,    502,     88,   4554,     13,\n",
       "              619,     88,   4554,    702,    264,   2118,   2456,    389,    595,\n",
       "             1126,     87,     13,   1207,  16632,     84,    702,    264,   2118,\n",
       "             2456,    389,    595,   1126,     87,     13,    576,   8084,  18927,\n",
       "              315,    502,     88,   4554,    374,    220,     21,     20,  14360,\n",
       "              576,  18927,    315,    537,    502,     88,   4554,    323,    595,\n",
       "             1126,     87,    374,    220,     17,     20,  14360,    576,  18927,\n",
       "              315,    502,     88,   4554,    323,    595,   1126,     87,    374,\n",
       "              220,     18,     15,  14360,   2160,    279,   6012,    315,    595,\n",
       "             1126,     87,   8131,    979,  44971,    502,     88,   4554,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,\n",
       "            51057,    264,    656,  95224,     11,  58059,   1879,    448,   1172,\n",
       "              279,   2701,   4682,     11,    323,   2041,    894,    650,  36591,\n",
       "             9363,    476,  58457,  11871,     25,  20035,  11994,   9448,    702,\n",
       "              264,   2118,   2456,    389,    279,    869,    323,    279,   8368,\n",
       "              278,     13,    576,   8368,    278,    702,    264,   2118,   2456,\n",
       "              389,  41850,     13,    576,    869,    702,    264,   2118,   2456,\n",
       "              389,  41850,     13,    576,   8084,  18927,    315,   3432,  11994,\n",
       "             9448,    374,    220,     16,     21,  14360,    576,  18927,    315,\n",
       "              537,   3432,  11994,   9448,    323,    279,  41850,    594,   4545,\n",
       "              374,    220,     17,     19,  14360,    576,  18927,    315,   3432,\n",
       "            11994,   9448,    323,    279,  41850,    594,   4545,    374,    220,\n",
       "               16,     17,  14360,   2160,    279,   6012,    315,    279,  41850,\n",
       "              594,   4545,   9155,    979,  44971,   3432,  11994,   9448,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151646,  51057,    264,    656,  95224,     11,  58059,   1879,\n",
       "              448,   1172,    279,   2701,   4682,     11,    323,   2041,    894,\n",
       "              650,  36591,   9363,    476,  58457,  11871,     25,    619,     88,\n",
       "             4554,    702,    264,   2118,   2456,    389,    595,   1126,     87,\n",
       "               13,    809,   7564,    702,    264,   2118,   2456,    389,    595,\n",
       "             1126,     87,     13,   1752,   1846,    879,    525,    537,    502,\n",
       "               88,   4554,     11,    279,  18927,    315,    595,   1126,     87,\n",
       "              374,    220,     19,     23,  14360,   1752,   1846,    879,    525,\n",
       "              502,     88,   4554,     11,    279,  18927,    315,    595,   1126,\n",
       "               87,    374,    220,     17,     22,  14360,   4841,    502,     88,\n",
       "             4554,   5263,    279,   6012,    315,    595,   1126,     87,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,\n",
       "              656,  95224,     11,  58059,   1879,    448,   1172,    279,   2701,\n",
       "             4682,     11,    323,   2041,    894,    650,  36591,   9363,    476,\n",
       "            58457,  11871,     25,   6867,   2978,  19578,   2639,    702,    264,\n",
       "             2118,   2456,    389,  30283,    594,   7194,   4680,    323,  30283,\n",
       "            28428,     13,   6267,   2971,    702,    264,   2118,   2456,    389,\n",
       "            30283,    594,   7194,   4680,    323,  30283,  28428,     13,  82388,\n",
       "              594,   7194,   4680,    702,    264,   2118,   2456,    389,  30283,\n",
       "            28428,     13,   6267,   2971,    374,    650,   5481,   2771,     13,\n",
       "              576,   8084,  18927,    315,  19578,   6554,    374,    220,     23,\n",
       "               16,  14360,    576,  18927,    315,  31695,     76,  10746,   6554,\n",
       "              323,   1550,  30283,  28428,    374,    220,     16,     18,  14360,\n",
       "              576,  18927,    315,  19578,   6554,    323,   1550,  30283,  28428,\n",
       "              374,    220,     19,     19,  14360,   2160,    279,   6012,    315,\n",
       "             1550,  30283,  28428,   9155,    979,  44971,  19578,   6554,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['yes', 'no', 'no', 'no', 'yes'],\n",
       "  'metadata': {'id': [6314, 23402, 15605, 27771, 8889],\n",
       "   'reasoning': ['Let Y = talent; X = appearance; V3 = fame.\\nX->V3,Y->V3\\nP(Y = 1 | X = 1, V3 = 1] - P(Y = 1 | V3 = 1)\\nP(Y=1 | X=1, V3=1) - (P(X=1) * P(Y=1 | X=1, V3=1) + P(X=0) * P(Y=1 | X=0, V3=1))\\nP(X=1) = 0.89\\nP(Y=1 | X=0, V3=0) = 0.99\\nP(Y=1 | X=0, V3=1) = 0.97\\nP(Y=1 | X=1, V3=0) = 0.99\\nP(Y=1 | X=1, V3=1) = 0.57\\n0.57 - (0.89*0.57 + 0.11*0.97) = -0.29\\n-0.29 < 0',\n",
       "    'Let V1 = yupt; V3 = qwiu; X = jyka; Y = kwox.\\nV1->V3,V1->X,X->Y,V3->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.65\\nP(Y=1, X=0=1) = 0.25\\nP(Y=1, X=1=1) = 0.30\\n0.30/0.65 - 0.25/0.35 = -0.27\\n-0.27 < 0',\n",
       "    'Let X = having visited England; V3 = the private; V2 = the corporal; Y = prisoner.\\nX->V3,X->V2,V2->Y,V3->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.16\\nP(Y=1, X=0=1) = 0.24\\nP(Y=1, X=1=1) = 0.12\\n0.12/0.16 - 0.24/0.84 = 0.47\\n0.47 > 0',\n",
       "    'Let V2 = yupt; X = jyka; Y = kwox.\\nX->Y,V2->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\nP(Y|X)\\nP(Y=1 | X=0) = 0.48\\nP(Y=1 | X=1) = 0.27\\n0.27 - 0.48 = -0.21\\n-0.21 < 0',\n",
       "    \"Let V2 = health condition; X = maternal smoking status; V3 = infant's birth weight; Y = infant mortality.\\nX->V3,V2->V3,X->Y,V2->Y,V3->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.81\\nP(Y=1, X=0=1) = 0.13\\nP(Y=1, X=1=1) = 0.44\\n0.44/0.81 - 0.13/0.19 = -0.16\\n-0.16 < 0\"],\n",
       "   'rung': [1, 1, 1, 2, 1],\n",
       "   'query_type': ['exp_away',\n",
       "    'correlation',\n",
       "    'correlation',\n",
       "    'ate',\n",
       "    'correlation'],\n",
       "   'graph_id': ['collision', 'diamondcut', 'diamond', 'fork', 'arrowhead'],\n",
       "   'story_id': ['celebrity',\n",
       "    'nonsense5',\n",
       "    'firing_squad',\n",
       "    'nonsense5',\n",
       "    'smoke_birthWeight'],\n",
       "   'question_property': ['hard',\n",
       "    'nonsense',\n",
       "    'anticommonsense',\n",
       "    'nonsense',\n",
       "    'hard'],\n",
       "   'formal_form': ['P(Y = 1 | X = 1, V3 = 1] - P(Y = 1 | V3 = 1)',\n",
       "    'P(Y | X)',\n",
       "    'P(Y | X)',\n",
       "    'E[Y | do(X = 1)] - E[Y | do(X = 0)]',\n",
       "    'P(Y | X)']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  35983,    311,   6468,\n",
       "            28963,    702,    264,   2118,   2456,    389,  32551,     13,  54507,\n",
       "              702,    264,   2118,   2456,    389,  32551,     13,    576,   8084,\n",
       "            18927,    315,  12094,  28963,    374,    220,     23,     24,  14360,\n",
       "             1752,   1251,    879,    653,    537,   6468,  28963,    323,    525,\n",
       "              537,  11245,     11,    279,  18927,    315,  10772,    374,    220,\n",
       "               24,     24,  14360,   1752,   1251,    879,    653,    537,   6468,\n",
       "            28963,    323,    525,  11245,     11,    279,  18927,    315,  10772,\n",
       "              374,    220,     24,     22,  14360,   1752,   1251,    879,   6468,\n",
       "            28963,    323,    525,    537,  11245,     11,    279,  18927,    315,\n",
       "            10772,    374,    220,     24,     24,  14360,   1752,   1251,    879,\n",
       "             6468,  28963,    323,    525,  11245,     11,    279,  18927,    315,\n",
       "            10772,    374,    220,     20,     22,  14360,   1416,    582,   1401,\n",
       "              518,   1251,    879,    525,  11245,     11,   1558,    279,   6012,\n",
       "              315,  10772,  18472,    979,  12094,  28963,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  94071,    702,    264,\n",
       "             2118,   2456,    389,  19754,    323,  60861,     13,  72599,    702,\n",
       "              264,   2118,   2456,    389,  19754,    323,  60861,     13,  52589,\n",
       "              702,    264,   2118,   2456,    389,  60861,     13,  72599,    374,\n",
       "              650,   5481,   2771,     13,   6730,    220,     16,     25,   1205,\n",
       "             1401,    518,   1246,  31948,  96203,    448,  60861,   1142,    553,\n",
       "             1142,   4092,    311,  19754,     13,   6730,    220,     17,     25,\n",
       "             1205,   1401,   5961,    518,   1246,  31948,  96203,    448,  60861,\n",
       "              304,   4586,     13,   2014,   3535,   1246,  31948,  21501,  60861,\n",
       "               11,    374,    432,    803,   4396,    311,    990,    279,   6730,\n",
       "              220,     16,   1091,   6730,    220,     17,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,\n",
       "            51057,    264,    656,  95224,     11,  58059,   1879,    448,   1172,\n",
       "              279,   2701,   4682,     11,    323,   2041,    894,    650,  36591,\n",
       "             9363,    476,  58457,  11871,     25,    619,     88,   4554,    702,\n",
       "              264,   2118,   2456,    389,    342,  18348,     79,    323,  31256,\n",
       "              706,     13,    472,     86,    706,    702,    264,   2118,   2456,\n",
       "              389,    326,    404,     70,     13,    479,  18348,     79,    702,\n",
       "              264,   2118,   2456,    389,    326,    404,     70,     13,   6730,\n",
       "              220,     16,     25,   1205,   1401,    518,   1246,    502,     88,\n",
       "             4554,  96203,    448,    326,    404,     70,   1142,    553,   1142,\n",
       "             4092,    311,    342,  18348,     79,     13,   6730,    220,     17,\n",
       "               25,   1205,   1401,   5961,    518,   1246,    502,     88,   4554,\n",
       "            96203,    448,    326,    404,     70,    304,   4586,     13,   2014,\n",
       "             3535,   1246,    502,     88,   4554,  21501,    326,    404,     70,\n",
       "               11,    374,    432,    803,   4396,    311,    990,    279,   6730,\n",
       "              220,     16,   1091,   6730,    220,     17,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  10751,    413,   3448,\n",
       "             2188,    702,    264,   2118,   2456,    389,  20956,  14132,    323,\n",
       "            13876,   6414,     13,   7659,   7024,  14132,    702,    264,   2118,\n",
       "             2456,    389,  13876,   6414,     13,   1752,   4143,    879,    525,\n",
       "              537,  20548,     11,    279,  18927,    315,  13876,   6414,    374,\n",
       "              220,     20,     15,  14360,   1752,   4143,    879,    525,  20548,\n",
       "               11,    279,  18927,    315,  13876,   6414,    374,    220,     18,\n",
       "               24,  14360,   1752,   4143,    879,    525,  20548,     11,   1035,\n",
       "              432,    387,    803,   4363,    311,   1490,  13876,   6414,    421,\n",
       "              279,   5458,   1030,   3949,    902,  50375,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151646,  51057,    264,    656,  95224,     11,  58059,\n",
       "             1879,    448,   1172,    279,   2701,   4682,     11,    323,   2041,\n",
       "              894,    650,  36591,   9363,    476,  58457,  11871,     25,  20035,\n",
       "              264,  10641,    702,    264,   2118,   2456,    389,   9947,     13,\n",
       "            27482,    702,    264,   2118,   2456,    389,  16107,     13,    576,\n",
       "             8084,  18927,    315,   3432,    264,  10641,    374,    220,     24,\n",
       "               22,  14360,    576,  18927,    315,    537,   3432,    264,  10641,\n",
       "              323,   1550,  16107,    374,    220,     16,  14360,    576,  18927,\n",
       "              315,   3432,    264,  10641,    323,   1550,  16107,    374,    220,\n",
       "               23,     18,  14360,   2160,    279,   6012,    315,   1550,  16107,\n",
       "             8131,    979,  44971,   3432,    264,  10641,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['yes', 'no', 'no', 'yes', 'yes'],\n",
       "  'metadata': {'id': [16706, 8089, 27242, 17252, 16896],\n",
       "   'reasoning': ['Let Y = talent; X = ability to speak english; V3 = fame.\\nX->V3,Y->V3\\nP(Y = 1 | X = 1, V3 = 1] - P(Y = 1 | V3 = 1)\\nP(Y=1 | X=1, V3=1) - (P(X=1) * P(Y=1 | X=1, V3=1) + P(X=0) * P(Y=1 | X=0, V3=1))\\nP(X=1) = 0.89\\nP(Y=1 | X=0, V3=0) = 0.99\\nP(Y=1 | X=0, V3=1) = 0.97\\nP(Y=1 | X=1, V3=0) = 0.99\\nP(Y=1 | X=1, V3=1) = 0.57\\n0.57 - (0.89*0.57 + 0.11*0.97) = -0.29\\n-0.29 < 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let X = encouragement level; V2 = studying habit; Y = brown eyes.\\nX->V2,X->Y,V2->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.50\\nP(Y=1 | X=1) = 0.39\\n0.39 - 0.50 = -0.10\\n-0.10 < 0',\n",
       "    'Let X = having a brother; V2 = skill; Y = salary.\\nX->V2,V2->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.97\\nP(Y=1, X=0=1) = 0.01\\nP(Y=1, X=1=1) = 0.83\\n0.83/0.97 - 0.01/0.03 = 0.34\\n0.34 > 0'],\n",
       "   'rung': [1, 2, 2, 3, 1],\n",
       "   'query_type': ['exp_away', 'backadj', 'backadj', 'ett', 'correlation'],\n",
       "   'graph_id': ['collision', 'arrowhead', 'diamond', 'mediation', 'chain'],\n",
       "   'story_id': ['celebrity',\n",
       "    'obesity_mortality',\n",
       "    'nonsense4',\n",
       "    'encouagement_program',\n",
       "    'college_salary'],\n",
       "   'question_property': ['anticommonsense',\n",
       "    'hard',\n",
       "    'nonsense',\n",
       "    'anticommonsense',\n",
       "    'anticommonsense'],\n",
       "   'formal_form': ['P(Y = 1 | X = 1, V3 = 1] - P(Y = 1 | V3 = 1)',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'P(Y | X)']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,   1863,    454,     71,\n",
       "              702,    264,   2118,   2456,    389,    435,    941,     80,    323,\n",
       "              856,   5120,     84,     13,    431,    941,     80,    702,    264,\n",
       "             2118,   2456,    389,    856,   5120,     84,     13,   1752,   1846,\n",
       "              879,    525,    537,   1147,    454,     71,    323,    525,    537,\n",
       "              435,    941,     80,     11,    279,  18927,    315,    856,   5120,\n",
       "               84,    374,    220,     16,     15,  14360,   1752,   1846,    879,\n",
       "              525,    537,   1147,    454,     71,    323,    525,    435,    941,\n",
       "               80,     11,    279,  18927,    315,    856,   5120,     84,    374,\n",
       "              220,     24,     18,  14360,   1752,   1846,    879,    525,   1147,\n",
       "              454,     71,    323,    525,    537,    435,    941,     80,     11,\n",
       "              279,  18927,    315,    856,   5120,     84,    374,    220,     22,\n",
       "               20,  14360,   1752,   1846,    879,    525,   1147,    454,     71,\n",
       "              323,    525,    435,    941,     80,     11,    279,  18927,    315,\n",
       "              856,   5120,     84,    374,    220,     23,     15,  14360,    576,\n",
       "             8084,  18927,    315,   1147,    454,     71,    374,    220,     18,\n",
       "               21,  14360,   1752,   1846,    879,    525,    435,    941,     80,\n",
       "               11,   1035,    432,    387,   2686,   4363,    311,   1490,    856,\n",
       "             5120,     84,    421,    279,   3842,    572,    537,    435,    941,\n",
       "               80,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151646,  51057,    264,    656,  95224,\n",
       "               11,  58059,   1879,    448,   1172,    279,   2701,   4682,     11,\n",
       "              323,   2041,    894,    650,  36591,   9363,    476,  58457,  11871,\n",
       "               25,  28217,    702,    264,   2118,   2456,    389,   9292,  76551,\n",
       "              323,  25293,   2639,     13,   1800,  25775,   2639,    702,    264,\n",
       "             2118,   2456,    389,   9292,  76551,    323,  25293,   2639,     13,\n",
       "             5887,  76551,    702,    264,   2118,   2456,    389,  25293,   2639,\n",
       "               13,   1800,  25775,   2639,    374,    650,   5481,   2771,     13,\n",
       "             1752,   7775,    879,    525,    537,   8593,     11,    279,  18927,\n",
       "              315,  25293,  25505,    374,    220,     18,     23,  14360,   1752,\n",
       "             7775,    879,    525,   8593,     11,    279,  18927,    315,  25293,\n",
       "            25505,    374,    220,     19,     15,  14360,   1752,   7775,    879,\n",
       "              525,   8593,     11,   1035,    432,    387,   2686,   4363,    311,\n",
       "             1490,  25293,  25505,    421,    279,   3842,   1030,    537,   1012,\n",
       "             8593,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,\n",
       "              656,  95224,     11,  58059,   1879,    448,   1172,    279,   2701,\n",
       "             4682,     11,    323,   2041,    894,    650,  36591,   9363,    476,\n",
       "            58457,  11871,     25,  11668,   2188,    702,    264,   2118,   2456,\n",
       "              389,   9947,     13,  27482,    702,    264,   2118,   2456,    389,\n",
       "            16107,     13,   1752,   1251,   2041,    264,   7770,   8381,     11,\n",
       "              279,  18927,    315,   1550,  16107,    374,    220,     18,     22,\n",
       "            14360,   1752,   1251,    448,    264,   7770,   8381,    476,   5080,\n",
       "               11,    279,  18927,    315,   1550,  16107,    374,    220,     19,\n",
       "               21,  14360,  12553,   6731,   2188,  47191,   7802,  16107,   1526,\n",
       "             9947,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151646,  51057,    264,    656,  95224,     11,\n",
       "            58059,   1879,    448,   1172,    279,   2701,   4682,     11,    323,\n",
       "             2041,    894,    650,  36591,   9363,    476,  58457,  11871,     25,\n",
       "             1863,    454,     71,    702,    264,   2118,   2456,    389,    348,\n",
       "              392,     81,    323,    289,    579,     75,     13,    467,    579,\n",
       "               75,    702,    264,   2118,   2456,    389,  29663,   8345,     13,\n",
       "              647,    392,     81,    702,    264,   2118,   2456,    389,  29663,\n",
       "             8345,     13,   1752,   1846,    879,    525,    537,   1147,    454,\n",
       "               71,     11,    279,  18927,    315,  29663,   8345,    374,    220,\n",
       "               19,     22,  14360,   1752,   1846,    879,    525,   1147,    454,\n",
       "               71,     11,    279,  18927,    315,  29663,   8345,    374,    220,\n",
       "               18,     16,  14360,  12553,   1147,    454,     71,  39546,   7802,\n",
       "            29663,   8345,   1526,    289,    579,     75,    323,    348,    392,\n",
       "               81,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,\n",
       "            51057,    264,    656,  95224,     11,  58059,   1879,    448,   1172,\n",
       "              279,   2701,   4682,     11,    323,   2041,    894,    650,  36591,\n",
       "             9363,    476,  58457,  11871,     25,  28217,    702,    264,   2118,\n",
       "             2456,    389,  19578,    323,  20622,   9387,     13,  72599,    702,\n",
       "              264,   2118,   2456,    389,  12183,  16539,     13,  23959,  16539,\n",
       "              702,    264,   2118,   2456,    389,  20622,   9387,     13,  28217,\n",
       "              374,    650,   5481,   2771,     13,   6730,    220,     16,     25,\n",
       "             1205,   1401,    518,   1246,  19578,  96203,    448,  20622,   9387,\n",
       "             1142,    553,   1142,   4092,    311,   9825,     13,   6730,    220,\n",
       "               17,     25,   1205,   1401,   5961,    518,   1246,  19578,  96203,\n",
       "              448,  20622,   9387,    304,   4586,     13,   2014,   3535,   1246,\n",
       "            19578,  21501,  20622,   9387,     11,    374,    432,    803,   4396,\n",
       "              311,    990,    279,   6730,    220,     16,   1091,   6730,    220,\n",
       "               17,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['yes', 'yes', 'no', 'no', 'yes'],\n",
       "  'metadata': {'id': [20886, 7139, 6508, 28594, 8962],\n",
       "   'reasoning': ['Let V1 = zuph; X = rixq; Y = xevu.\\nV1->X,V1->Y,X->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\n\\\\sum_{V1=v} P(V1=v|X=1)*[P(Y=1|V1=v,X=1) - P(Y=1|V1=v, X=0)]\\nP(Y=1 | V1=0, X=0) = 0.10\\nP(Y=1 | V1=0, X=1) = 0.93\\nP(Y=1 | V1=1, X=0) = 0.75\\nP(Y=1 | V1=1, X=1) = 0.80\\nP(V1=1) = 0.36\\n0.64 * (0.93 - 0.10) + 0.36 * (0.80 - 0.75) = 0.52\\n0.52 > 0',\n",
       "    'Let V2 = residency status; X = gender; V3 = department competitiveness; Y = admission status.\\nX->V3,V2->V3,X->Y,V2->Y,V3->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.38\\nP(Y=1 | X=1) = 0.40\\n0.40 - 0.38 = 0.02\\n0.02 > 0',\n",
       "    'Let X = education level; V2 = skill; Y = salary.\\nX->V2,V2->Y\\nE[Y_{X=0, V2=1} - Y_{X=0, V2=0}]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.37\\nP(Y=1 | X=1) = 0.46\\n0.46 - 0.37 = 0.09\\n0.09 > 0',\n",
       "    'Let X = zuph; V3 = vubr; V2 = wibl; Y = uvzi.\\nX->V3,X->V2,V2->Y,V3->Y\\nE[Y_{X=0, V2=1, V3=1} - Y_{X=0, V2=0, V3=0}]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.47\\nP(Y=1 | X=1) = 0.31\\n0.31 - 0.47 = -0.17\\n-0.17 < 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan'],\n",
       "   'rung': [3, 3, 3, 3, 2],\n",
       "   'query_type': ['ett', 'ett', 'nie', 'nie', 'backadj'],\n",
       "   'graph_id': ['confounding', 'arrowhead', 'chain', 'diamond', 'frontdoor'],\n",
       "   'story_id': ['nonsense0',\n",
       "    'gender_admission_state',\n",
       "    'college_salary',\n",
       "    'nonsense7',\n",
       "    'smoking_frontdoor'],\n",
       "   'question_property': ['nonsense', 'hard', 'hard', 'nonsense', 'hard'],\n",
       "   'formal_form': ['E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]',\n",
       "    'E[Y_{X=0, V2=1, V3=1} - Y_{X=0, V2=0, V3=0}]',\n",
       "    '[backdoor adjustment set for Y given X]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  35983,    311,   6468,\n",
       "            28963,    702,    264,   2118,   2456,    389,    279,  13638,     13,\n",
       "              576,  77220,    702,    264,   2118,   2456,    389,    279,  13638,\n",
       "               13,    576,   8084,  18927,    315,  12094,  28963,    374,    220,\n",
       "               24,     15,  14360,    576,  18927,    315,    537,  12094,  28963,\n",
       "              323,    279,  13638,    389,   3940,    374,    220,     23,  14360,\n",
       "              576,  18927,    315,  12094,  28963,    323,    279,  13638,    389,\n",
       "             3940,    374,    220,     19,     21,  14360,   2160,    279,   6012,\n",
       "              315,    279,  13638,    389,   3940,   8131,    979,  44971,  12094,\n",
       "            28963,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151646,  51057,    264,    656,  95224,\n",
       "               11,  58059,   1879,    448,   1172,    279,   2701,   4682,     11,\n",
       "              323,   2041,    894,    650,  36591,   9363,    476,  58457,  11871,\n",
       "               25,  12156,    702,    264,   2118,   2456,    389,   7538,    323,\n",
       "             6645,     13,  10567,    702,    264,   2118,   2456,    389,   9364,\n",
       "               13,  10560,    702,    264,   2118,   2456,    389,   9364,     13,\n",
       "              576,   8084,  18927,    315,   6645,  15971,    279,  34408,   6524,\n",
       "              374,    220,     20,     20,  14360,   1752,  19680,    879,   1513,\n",
       "              944,   1841,  34408,  11931,     11,    279,  18927,    315,   9364,\n",
       "             1660,  13895,    374,    220,     16,     23,  14360,   1752,  19680,\n",
       "              879,   1841,  34408,  11931,     11,    279,  18927,    315,   9364,\n",
       "             1660,  13895,    374,    220,     21,     16,  14360,   2160,   9364,\n",
       "             1660,  13895,    803,   4363,   1091,   9364,    537,   1660,  13895,\n",
       "             8084,     30],\n",
       "          [151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  73359,    702,    264,\n",
       "             2118,   2456,    389,   7403,    323,  16624,   8866,     13,  42408,\n",
       "              702,    264,   2118,   2456,    389,  16624,   8866,     13,   1752,\n",
       "            60854,    429,   1513,    944,    738,    279,  16624,    323,  38620,\n",
       "              429,   1513,    944,    738,    279,  16624,     11,    279,  18927,\n",
       "              315,  67043,  16624,    374,    220,     24,     20,  14360,   1752,\n",
       "            60854,    429,   1513,    944,    738,    279,  16624,    323,  38620,\n",
       "              429,    738,    279,  16624,     11,    279,  18927,    315,  67043,\n",
       "            16624,    374,    220,     20,     17,  14360,   1752,  60854,    429,\n",
       "              738,    279,  16624,    323,  38620,    429,   1513,    944,    738,\n",
       "              279,  16624,     11,    279,  18927,    315,  67043,  16624,    374,\n",
       "              220,     21,     18,  14360,   1752,  60854,    429,    738,    279,\n",
       "            16624,    323,  38620,    429,    738,    279,  16624,     11,    279,\n",
       "            18927,    315,  67043,  16624,    374,    220,     16,     18,  14360,\n",
       "             1752,  60854,    429,   1513,    944,    738,    279,  16624,     11,\n",
       "              279,  18927,    315,  16624,    738,    553,   7403,    374,    220,\n",
       "               17,     18,  14360,   1752,  60854,    429,    738,    279,  16624,\n",
       "               11,    279,  18927,    315,  16624,    738,    553,   7403,    374,\n",
       "              220,     20,     24,  14360,   1416,    582,  57717,    279,  76573,\n",
       "             2456,   1526,   7403,     11,   1035,   9972,  47191,   7802,  16624,\n",
       "             8866,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,\n",
       "              656,  95224,     11,  58059,   1879,    448,   1172,    279,   2701,\n",
       "             4682,     11,    323,   2041,    894,    650,  36591,   9363,    476,\n",
       "            58457,  11871,     25,   1599,     68,   3334,    702,    264,   2118,\n",
       "             2456,    389,    259,   3172,     85,    323,    342,  18348,     79,\n",
       "               13,    350,   3172,     85,    702,    264,   2118,   2456,    389,\n",
       "              342,  18348,     79,     13,   1752,   1846,    879,    525,    537,\n",
       "            82790,   3334,     11,    279,  18927,    315,    342,  18348,     79,\n",
       "              374,    220,     22,     17,  14360,   1752,   1846,    879,    525,\n",
       "            82790,   3334,     11,    279,  18927,    315,    342,  18348,     79,\n",
       "              374,    220,     22,  14360,   1752,   1846,    879,    525,  82790,\n",
       "             3334,     11,   1035,    432,    387,    803,   4363,    311,   1490,\n",
       "              342,  18348,     79,    421,    279,   3842,    572,    537,  82790,\n",
       "             3334,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151646,  51057,    264,    656,  95224,     11,  58059,   1879,\n",
       "              448,   1172,    279,   2701,   4682,     11,    323,   2041,    894,\n",
       "              650,  36591,   9363,    476,  58457,  11871,     25,    809,   7564,\n",
       "              702,    264,   2118,   2456,    389,    502,     88,   4554,    323,\n",
       "              595,   1126,     87,     13,    619,     88,   4554,    702,    264,\n",
       "             2118,   2456,    389,    595,   1126,     87,     13,   1205,   1414,\n",
       "              429,    379,   7564,  11137,    537,    502,     88,   4554,     13,\n",
       "              379,   7564,    323,    502,     88,   4554,  11137,    595,   1126,\n",
       "               87,     13,   1205,  13166,    458,   3842,    374,    379,   7564,\n",
       "               13,  18885,    458,   3842,    374,    537,    595,   1126,     87,\n",
       "              421,    502,     88,   4554,   4518,    315,    537,    502,     88,\n",
       "             4554,     30]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'yes', 'yes', 'no'],\n",
       "  'metadata': {'id': [15860, 332, 4835, 21823, 30335],\n",
       "   'reasoning': ['Let V2 = the smoker; X = ability to speak english; Y = the forest.\\nX->Y,V2->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.90\\nP(Y=1, X=0=1) = 0.08\\nP(Y=1, X=1=1) = 0.46\\n0.46/0.90 - 0.08/0.10 = -0.32\\n-0.32 < 0',\n",
       "    'Let V1 = CEO; V3 = director; X = manager; Y = employee.\\nV1->V3,V1->X,X->Y,V3->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.55\\nP(Y=1 | X=0) = 0.18\\nP(Y=1 | X=1) = 0.61\\n0.55*0.61 - 0.45*0.18 = 0.42\\n0.42 > 0',\n",
       "    'Let X = husband; V2 = wife; Y = alarm clock.\\nX->V2,X->Y,V2->Y\\nE[Y_{X=1, V2=0} - Y_{X=0, V2=0}]\\n\\\\sum_{V2=v} P(V2=v|X=0)*[P(Y=1|X=1,V2=v) - P(Y=1|X=0, V2=v)]\\nP(Y=1 | X=0, V2=0) = 0.95\\nP(Y=1 | X=0, V2=1) = 0.52\\nP(Y=1 | X=1, V2=0) = 0.63\\nP(Y=1 | X=1, V2=1) = 0.13\\nP(V2=1 | X=0) = 0.23\\nP(V2=1 | X=1) = 0.59\\n0.23 * (0.13 - 0.63) + 0.59 * (0.52 - 0.95) = -0.34\\n-0.34 < 0',\n",
       "    'Let X = xevo; V2 = tijv; Y = gyzp.\\nX->V2,X->Y,V2->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.72\\nP(Y=1 | X=1) = 0.07\\n0.07 - 0.72 = -0.65\\n-0.65 < 0',\n",
       "    'Let V1 = yupt; X = jyka; Y = kwox.\\nV1->X,V1->Y,X->Y\\nY_{X=1} = 0 | V1=1\\nSolve for Y, given the evidence and the action\\nV1 = 1\\nX = not V1\\nY = V1 and X\\nY = 1 = 1 and 1\\n0'],\n",
       "   'rung': [1, 1, 3, 3, 3],\n",
       "   'query_type': ['correlation',\n",
       "    'marginal',\n",
       "    'nde',\n",
       "    'ett',\n",
       "    'det-counterfactual'],\n",
       "   'graph_id': ['fork', 'diamondcut', 'mediation', 'mediation', 'confounding'],\n",
       "   'story_id': ['forest_fire',\n",
       "    'firing_employee',\n",
       "    'alarm',\n",
       "    'nonsense2',\n",
       "    'nonsense5'],\n",
       "   'question_property': ['anticommonsense',\n",
       "    'easy',\n",
       "    'hard',\n",
       "    'nonsense',\n",
       "    'nonsense'],\n",
       "   'formal_form': ['P(Y | X)',\n",
       "    'P(Y)',\n",
       "    'E[Y_{X=1, V2=0} - Y_{X=0, V2=0}]',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'Y_{X=1} = 0 | V1=1']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,   5120,     84,     30],\n",
       "          [151643, 151643, 151643,  ...,    327,     84,     30],\n",
       "          [151646,  51057,    264,  ...,  31256,    706,     30],\n",
       "          [151643, 151643, 151643,  ...,    220,     17,     30],\n",
       "          [151643, 151643, 151643,  ...,    220,     17,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'no', 'yes', 'no'],\n",
       "  'metadata': {'id': [22183, 30801, 24604, 4539, 13039],\n",
       "   'reasoning': ['Let Y = xevu; X = yomx; V3 = gwet.\\nX->V3,Y->V3\\nE[Y = 1 | do(X = 1), V3 = 1] - E[Y = 1 | do(X = 0), V3 = 1]\\nX and Y do not affect each other.\\n\\n0\\nno',\n",
       "    'Let V2 = hwax; V1 = kraz; X = pexu; Y = rukz.\\nV1->X,V2->X,V1->Y,X->Y\\nY_{X=0} = 0 | V2=0, V1=1\\nSolve for Y, given the evidence and the action\\nV2 = 0\\nV1 = 1\\nX = V1 and V2\\nY = V1 or X\\nY = 1 = 1 or 0\\n0',\n",
       "    'Let V2 = kraz; X = pexu; V3 = hwax; Y = rukz.\\nX->V3,V2->V3,X->Y,V2->Y,V3->Y\\nE[Y_{X=0, V3=1} - Y_{X=0, V3=0}]\\n\\\\sum_{V3=v} [\\\\sum_{V2=k} P(Y=1|X=0,V3=v)*[P(V3=v|X=1,V2=k)-P(V3=v|X=0,V2=k)]*P(V2=k)]\\nP(Y=1 | X=0, V3=0) = 0.45\\nP(Y=1 | X=0, V3=1) = 0.98\\nP(Y=1 | X=1, V3=0) = 0.86\\nP(Y=1 | X=1, V3=1) = 0.62\\nP(V3=1 | X=0, V2=0) = 0.58\\nP(V3=1 | X=0, V2=1) = 0.02\\nP(V3=1 | X=1, V2=0) = 0.62\\nP(V3=1 | X=1, V2=1) = 0.19\\nP(V2=1) = 0.84\\n0.84 * 0.98 * (0.19 - 0.62)+ 0.16 * 0.98 * (0.02 - 0.58)= 0.08\\n0.08 > 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan'],\n",
       "   'rung': [2, 3, 3, 2, 2],\n",
       "   'query_type': ['collider_bias',\n",
       "    'det-counterfactual',\n",
       "    'nie',\n",
       "    'backadj',\n",
       "    'backadj'],\n",
       "   'graph_id': ['collision', 'IV', 'arrowhead', 'chain', 'collision'],\n",
       "   'story_id': ['nonsense3',\n",
       "    'nonsense8',\n",
       "    'nonsense8',\n",
       "    'smoking_tar_cancer',\n",
       "    'hospitalization'],\n",
       "   'question_property': ['nonsense',\n",
       "    'nonsense',\n",
       "    'nonsense',\n",
       "    'easy',\n",
       "    'anticommonsense'],\n",
       "   'formal_form': ['E[Y = 1 | do(X = 1), V3 = 1] - E[Y = 1 | do(X = 0), V3 = 1]',\n",
       "    'Y_{X=0} = 0 | V2=0, V1=1',\n",
       "    'E[Y_{X=0, V3=1} - Y_{X=0, V3=0}]',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    '[backdoor adjustment set for Y given X]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,   7770,   8381,     30],\n",
       "          [151643, 151643, 151643,  ...,     84,   8084,     30],\n",
       "          [151646,  51057,    264,  ...,   1126,     87,     30],\n",
       "          [151643, 151643, 151643,  ...,   8345,   8084,     30],\n",
       "          [151643, 151643, 151643,  ...,    220,     17,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       "  'labels': ['yes', 'yes', 'yes', 'yes', 'yes'],\n",
       "  'metadata': {'id': [6487, 21027, 25787, 24079, 11223],\n",
       "   'reasoning': ['Let X = education level; V2 = skill; Y = salary.\\nX->V2,V2->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.12\\nP(Y=1 | X=1) = 0.26\\n0.26 - 0.12 = 0.14\\n0.14 > 0',\n",
       "    'Let V2 = zuph; V1 = swoy; X = rixq; Y = xevu.\\nV1->X,V2->X,V1->Y,X->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.62\\nP(Y=1 | X=0) = 0.17\\nP(Y=1 | X=1) = 0.31\\n0.62*0.31 - 0.38*0.17 = 0.25\\n0.25 > 0',\n",
       "    'Let V1 = muvy; X = kwox; V3 = swoq; Y = kwoz.\\nV1->X,X->V3,V1->Y,V3->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\n\\\\sum_{V3=v}P(Y=1|X=1,V3=v)*[P(V3=v|X=1)-P(V3=v|X=0)]\\nP(Y=1 | X=0, V3=0) = 0.36\\nP(Y=1 | X=0, V3=1) = 0.69\\nP(Y=1 | X=1, V3=0) = 0.31\\nP(Y=1 | X=1, V3=1) = 0.69\\nP(V3=1 | X=0) = 0.18\\nP(V3=1 | X=1) = 0.05\\n0.69 * (0.05 - 0.18) + 0.69 * (0.95 - 0.82) = -0.05\\n-0.05 < 0',\n",
       "    'Let V1 = vubr; X = zuph; V3 = wibl; Y = uvzi.\\nV1->X,X->V3,V1->Y,V3->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.69\\nP(Y=1 | X=0) = 0.36\\nP(Y=1 | X=1) = 0.36\\n0.69*0.36 - 0.31*0.36 = 0.36\\n0.36 > 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan'],\n",
       "   'rung': [3, 1, 3, 1, 2],\n",
       "   'query_type': ['ett', 'marginal', 'ett', 'marginal', 'backadj'],\n",
       "   'graph_id': ['chain', 'IV', 'frontdoor', 'frontdoor', 'fork'],\n",
       "   'story_id': ['college_salary',\n",
       "    'nonsense0',\n",
       "    'nonsense1',\n",
       "    'nonsense7',\n",
       "    'getting_late'],\n",
       "   'question_property': ['hard',\n",
       "    'nonsense',\n",
       "    'nonsense',\n",
       "    'nonsense',\n",
       "    'anticommonsense'],\n",
       "   'formal_form': ['E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'P(Y)',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'P(Y)',\n",
       "    '[backdoor adjustment set for Y given X]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,   7449,   8084,     30],\n",
       "          [151643, 151643, 151643,  ...,    941,     80,     30],\n",
       "          [151646,  51057,    264,  ...,   3590,   2639,     30],\n",
       "          [151643, 151643, 151643,  ...,    327,     84,     30],\n",
       "          [151643, 151643, 151643,  ...,  13675,   8084,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       "  'labels': ['no', 'yes', 'yes', 'yes', 'yes'],\n",
       "  'metadata': {'id': [8630, 29702, 7651, 30753, 15709],\n",
       "   'reasoning': ['Let V1 = kidney stone size; X = treatment; Y = recovery.\\nV1->X,V1->Y,X->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.63\\nP(Y=1 | X=0) = 0.25\\nP(Y=1 | X=1) = 0.62\\n0.63*0.62 - 0.37*0.25 = 0.48\\n0.48 > 0',\n",
       "    'Let V1 = zuph; V3 = swoy; X = rixq; Y = xevu.\\nV1->V3,V1->X,X->Y,V3->Y\\nY_{X=1} = 1 | V1=0\\nSolve for Y, given the evidence and the action\\nV1 = 0\\nX = V1\\nV3 = not X\\nY = X and V3\\nY = 1 = 1 and 1\\n1',\n",
       "    \"Let V2 = other unobserved factors; X = parents' intelligence; V3 = parents' social status; Y = child's intelligence.\\nX->V3,V2->V3,X->Y,V2->Y,V3->Y\\nE[Y_{X=0, V3=1} - Y_{X=0, V3=0}]\\n\\\\sum_{V3=v} [\\\\sum_{V2=k} P(Y=1|X=0,V3=v)*[P(V3=v|X=1,V2=k)-P(V3=v|X=0,V2=k)]*P(V2=k)]\\nP(Y=1 | X=0, V3=0) = 0.77\\nP(Y=1 | X=0, V3=1) = 0.65\\nP(Y=1 | X=1, V3=0) = 0.28\\nP(Y=1 | X=1, V3=1) = 0.17\\nP(V3=1 | X=0, V2=0) = 0.93\\nP(V3=1 | X=0, V2=1) = 0.63\\nP(V3=1 | X=1, V2=0) = 0.57\\nP(V3=1 | X=1, V2=1) = 0.29\\nP(V2=1) = 0.44\\n0.44 * 0.65 * (0.29 - 0.57)+ 0.56 * 0.65 * (0.63 - 0.93)= 0.04\\n0.04 > 0\",\n",
       "    'Let V1 = hwax; X = pexu; Y = rukz.\\nV1->X,V1->Y,X->Y\\nY_{X=1} = 0 | V1=0\\nSolve for Y, given the evidence and the action\\nV1 = 0\\nX = not V1\\nY = V1 and X\\nY = 0 = 0 and 1\\n1',\n",
       "    'Let X = having visited England; V3 = the private; V2 = the corporal; Y = prisoner.\\nX->V3,X->V2,V2->Y,V3->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.71\\nP(Y=1 | X=0) = 0.31\\nP(Y=1 | X=1) = 0.72\\n0.71*0.72 - 0.29*0.31 = 0.60\\n0.60 > 0'],\n",
       "   'rung': [1, 3, 3, 3, 1],\n",
       "   'query_type': ['marginal',\n",
       "    'det-counterfactual',\n",
       "    'nie',\n",
       "    'det-counterfactual',\n",
       "    'marginal'],\n",
       "   'graph_id': ['confounding',\n",
       "    'diamondcut',\n",
       "    'arrowhead',\n",
       "    'confounding',\n",
       "    'diamond'],\n",
       "   'story_id': ['simpson_kidneystone',\n",
       "    'nonsense0',\n",
       "    'nature_vs_nurture',\n",
       "    'nonsense8',\n",
       "    'firing_squad'],\n",
       "   'question_property': ['hard',\n",
       "    'nonsense',\n",
       "    'hard',\n",
       "    'nonsense',\n",
       "    'anticommonsense'],\n",
       "   'formal_form': ['P(Y)',\n",
       "    'Y_{X=1} = 1 | V1=0',\n",
       "    'E[Y_{X=0, V3=1} - Y_{X=0, V3=0}]',\n",
       "    'Y_{X=1} = 0 | V1=0',\n",
       "    'P(Y)']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,    517,   8084,     30],\n",
       "          [151643, 151643, 151643,  ...,   2588,  63892,     30],\n",
       "          [151643, 151643, 151643,  ...,    264,  10641,     30],\n",
       "          [151646,  51057,    264,  ...,  31256,    706,     30],\n",
       "          [151643, 151643, 151643,  ...,    454,     71,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'yes', 'no', 'no'],\n",
       "  'metadata': {'id': [14351, 12370, 14032, 27112, 30887],\n",
       "   'reasoning': ['Let V1 = pre-conditions; X = vaccination; Y = lactose intolerance.\\nV1->X,V1->Y,X->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.59\\nP(Y=1 | X=0) = 0.59\\nP(Y=1 | X=1) = 0.34\\n0.59*0.34 - 0.41*0.59 = 0.44\\n0.44 > 0',\n",
       "    'Let Y = effort; X = listening to jazz; V3 = elite institution admission status.\\nX->V3,Y->V3\\nE[Y = 1 | do(X = 1), V3 = 1] - E[Y = 1 | do(X = 0), V3 = 1]\\nX and Y do not affect each other.\\n\\n0\\nno',\n",
       "    'Let V1 = gender; X = having a brother; Y = recovery.\\nV1->X,V1->Y,X->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.41\\nP(Y=1, X=0=1) = 0.42\\nP(Y=1, X=1=1) = 0.10\\n0.10/0.41 - 0.42/0.59 = -0.47\\n-0.47 < 0',\n",
       "    'Let V1 = gyzp; X = jyka; V3 = hwax; Y = lirg.\\nV1->X,X->V3,V1->Y,V3->Y\\nE[Y_{X=0, V3=1} - Y_{X=0, V3=0}]\\n\\\\sum_{V3 = v} [P(V3 = v|X = 1) - P(V3 = v|X = 0)] * [\\\\sum_{X = h} P(Y = 1|X = h,V3 = v)*P(X = h)]\\nP(V3=1 | X=0) = 0.47\\nP(V3=1 | X=1) = 0.06\\nP(Y=1 | X=0, V3=0) = 0.57\\nP(Y=1 | X=0, V3=1) = 0.29\\nP(Y=1 | X=1, V3=0) = 0.63\\nP(Y=1 | X=1, V3=1) = 0.30\\nP(X=1) = 0.21\\n0.06 - 0.47 * (0.30 * 0.21 + 0.29 * 0.79)= 0.12\\n0.12 > 0',\n",
       "    'Let X = zuph; V2 = jyka; Y = glimx.\\nX->V2,V2->Y\\nY_{X=1} = 0 | \\nSolve for Y, given the evidence and the action\\nV2 = X\\nY = V2\\nY = [1] = 1\\n0'],\n",
       "   'rung': [1, 2, 1, 3, 3],\n",
       "   'query_type': ['marginal',\n",
       "    'collider_bias',\n",
       "    'correlation',\n",
       "    'nie',\n",
       "    'det-counterfactual'],\n",
       "   'graph_id': ['confounding',\n",
       "    'collision',\n",
       "    'confounding',\n",
       "    'frontdoor',\n",
       "    'chain'],\n",
       "   'story_id': ['simpson_vaccine',\n",
       "    'elite_students',\n",
       "    'simpson_drug',\n",
       "    'nonsense4',\n",
       "    'nonsense9'],\n",
       "   'question_property': ['anticommonsense',\n",
       "    'anticommonsense',\n",
       "    'anticommonsense',\n",
       "    'nonsense',\n",
       "    'nonsense'],\n",
       "   'formal_form': ['P(Y)',\n",
       "    'E[Y = 1 | do(X = 1), V3 = 1] - E[Y = 1 | do(X = 0), V3 = 1]',\n",
       "    'P(Y | X)',\n",
       "    'E[Y_{X=0, V3=1} - Y_{X=0, V3=0}]',\n",
       "    'Y_{X=1} = 0 | ']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,\n",
       "            51057,    264,    656,  95224,     11,  58059,   1879,    448,   1172,\n",
       "              279,   2701,   4682,     11,    323,   2041,    894,    650,  36591,\n",
       "             9363,    476,  58457,  11871,     25,    619,     88,   4554,    702,\n",
       "              264,   2118,   2456,    389,    379,   7564,     13,    809,   7564,\n",
       "              702,    264,   2118,   2456,    389,    595,   1126,     87,     13,\n",
       "             1752,   1846,    879,    525,    537,    502,     88,   4554,     11,\n",
       "              279,  18927,    315,    595,   1126,     87,    374,    220,     22,\n",
       "               22,  14360,   1752,   1846,    879,    525,    502,     88,   4554,\n",
       "               11,    279,  18927,    315,    595,   1126,     87,    374,    220,\n",
       "               17,     19,  14360,   1752,   1846,    879,    525,    502,     88,\n",
       "             4554,     11,   1035,    432,    387,    803,   4363,    311,   1490,\n",
       "              595,   1126,     87,    421,    279,   3842,    572,    537,    502,\n",
       "               88,   4554,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,\n",
       "            51057,    264,    656,  95224,     11,  58059,   1879,    448,   1172,\n",
       "              279,   2701,   4682,     11,    323,   2041,    894,    650,  36591,\n",
       "             9363,    476,  58457,  11871,     25,    472,     86,    706,    702,\n",
       "              264,   2118,   2456,    389,    342,  18348,     79,    323,    502,\n",
       "               88,   4554,     13,    619,     88,   4554,    702,    264,   2118,\n",
       "             2456,    389,    326,    404,     70,     13,    479,  18348,     79,\n",
       "              702,    264,   2118,   2456,    389,    326,    404,     70,     13,\n",
       "              576,   8084,  18927,    315,    502,     88,   4554,    374,    220,\n",
       "               22,     22,  14360,    576,  18927,    315,    537,    502,     88,\n",
       "             4554,    323,    326,    404,     70,    374,    220,     16,     15,\n",
       "            14360,    576,  18927,    315,    502,     88,   4554,    323,    326,\n",
       "              404,     70,    374,    220,     20,     20,  14360,   2160,    279,\n",
       "             6012,    315,    326,    404,     70,   9155,    979,  44971,    502,\n",
       "               88,   4554,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151646,  51057,    264,    656,  95224,     11,\n",
       "            58059,   1879,    448,   1172,    279,   2701,   4682,     11,    323,\n",
       "             2041,    894,    650,  36591,   9363,    476,  58457,  11871,     25,\n",
       "            72599,    702,    264,   2118,   2456,    389,  12183,  16539,     13,\n",
       "            23959,  16539,    702,    264,   2118,   2456,    389,  20622,   9387,\n",
       "               13,   1752,  31695,     76,  40681,     11,    279,  18927,    315,\n",
       "            20622,   9387,    374,    220,     17,     24,  14360,   1752,  56657,\n",
       "               11,    279,  18927,    315,  20622,   9387,    374,    220,     21,\n",
       "               19,  14360,  12553,  19578,  47191,   7802,  20622,   9387,   1526,\n",
       "            12183,  16539,     30],\n",
       "          [151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,   1230,   5481,   2771,\n",
       "             2335,    795,    388,    702,    264,   2118,   2456,    389,   5506,\n",
       "             4429,    323,   3457,    377,    642,     13,  30869,  16319,    702,\n",
       "              264,   2118,   2456,    389,   5506,   4429,     13,  25109,   4429,\n",
       "              702,    264,   2118,   2456,    389,   3457,    377,    642,     13,\n",
       "             1230,   5481,   2771,   2335,    795,    388,    374,    650,   5481,\n",
       "             2771,     13,    576,   8084,  18927,    315,   4633,    315,    678,\n",
       "            12607,  10975,    374,    220,     17,     17,  14360,    576,  18927,\n",
       "              315,    537,   4633,    315,    894,  12607,  10975,    323,   3457,\n",
       "              377,    642,    374,    220,     17,     19,  14360,    576,  18927,\n",
       "              315,   4633,    315,    678,  12607,  10975,    323,   3457,    377,\n",
       "              642,    374,    220,     21,  14360,   2160,    279,   6012,    315,\n",
       "             3457,    377,    642,   9155,    979,  44971,   4633,    315,    678,\n",
       "            12607,  10975,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,\n",
       "              264,    656,  95224,     11,  58059,   1879,    448,   1172,    279,\n",
       "             2701,   4682,     11,    323,   2041,    894,    650,  36591,   9363,\n",
       "              476,  58457,  11871,     25,   1863,    454,     73,    702,    264,\n",
       "             2118,   2456,    389,  30784,    824,    323,    379,   7564,     13,\n",
       "              809,   7564,    702,    264,   2118,   2456,    389,  11834,     85,\n",
       "               80,     13,   1599,     88,    824,    702,    264,   2118,   2456,\n",
       "              389,  11834,     85,     80,     13,   6730,    220,     16,     25,\n",
       "             1205,   1401,   5961,    518,   1246,    379,   7564,  96203,    448,\n",
       "            11834,     85,     80,    304,   4586,     13,   6730,    220,     17,\n",
       "               25,   1205,   1401,    518,    419,  25588,   1142,    553,   1142,\n",
       "             4092,    311,  30784,    824,     13,   2014,   3535,   1246,    379,\n",
       "             7564,  21501,  11834,     85,     80,     11,    374,    432,    803,\n",
       "             4396,    311,    990,    279,   6730,    220,     16,   1091,   6730,\n",
       "              220,     17,     30]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['yes', 'no', 'no', 'yes', 'no'],\n",
       "  'metadata': {'id': [23044, 27282, 9245, 12066, 28144],\n",
       "   'reasoning': ['Let X = jyka; V2 = yupt; Y = kwox.\\nX->V2,V2->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.77\\nP(Y=1 | X=1) = 0.24\\n0.24 - 0.77 = -0.54\\n-0.54 < 0',\n",
       "    'Let V1 = hwax; V3 = gyzp; X = jyka; Y = lirg.\\nV1->V3,V1->X,X->Y,V3->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.77\\nP(Y=1, X=0=1) = 0.10\\nP(Y=1, X=1=1) = 0.55\\n0.55/0.77 - 0.10/0.23 = 0.25\\n0.25 > 0',\n",
       "    'Let X = smoking; V2 = tar deposit; Y = lung cancer.\\nX->V2,V2->Y\\nE[Y_{X=0, V2=1} - Y_{X=0, V2=0}]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.29\\nP(Y=1 | X=1) = 0.64\\n0.64 - 0.29 = 0.35\\n0.35 > 0',\n",
       "    'Let V2 = treatment assignment; V1 = unobserved confounders; X = drug taken; Y = freckles.\\nV1->X,V2->X,V1->Y,X->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.22\\nP(Y=1, X=0=1) = 0.24\\nP(Y=1, X=1=1) = 0.06\\n0.06/0.22 - 0.24/0.78 = -0.04\\n-0.04 < 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan'],\n",
       "   'rung': [3, 1, 3, 1, 2],\n",
       "   'query_type': ['ett', 'correlation', 'nie', 'correlation', 'backadj'],\n",
       "   'graph_id': ['chain', 'diamondcut', 'chain', 'IV', 'diamondcut'],\n",
       "   'story_id': ['nonsense5',\n",
       "    'nonsense4',\n",
       "    'smoking_tar_cancer',\n",
       "    'cholesterol',\n",
       "    'nonsense6'],\n",
       "   'question_property': ['nonsense',\n",
       "    'nonsense',\n",
       "    'hard',\n",
       "    'anticommonsense',\n",
       "    'nonsense'],\n",
       "   'formal_form': ['E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'P(Y | X)',\n",
       "    'E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]',\n",
       "    'P(Y | X)',\n",
       "    '[backdoor adjustment set for Y given X]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  28217,    702,    264,\n",
       "             2118,   2456,    389,  19578,    323,  49833,  59654,     13,  72599,\n",
       "              702,    264,   2118,   2456,    389,  12183,  16539,     13,  23959,\n",
       "            16539,    702,    264,   2118,   2456,    389,  49833,  59654,     13,\n",
       "            28217,    374,    650,   5481,   2771,     13,   1752,  31695,     76,\n",
       "            40681,    323,    448,    902,  12183,  16539,     11,    279,  18927,\n",
       "              315,   1660,  56496,    311,  89077,    374,    220,     17,     20,\n",
       "            14360,   1752,  31695,     76,  40681,    323,    448,   1550,  12183,\n",
       "            16539,     11,    279,  18927,    315,   1660,  56496,    311,  89077,\n",
       "              374,    220,     20,     22,  14360,   1752,  56657,    323,    448,\n",
       "              902,  12183,  16539,     11,    279,  18927,    315,   1660,  56496,\n",
       "              311,  89077,    374,    220,     17,     21,  14360,   1752,  56657,\n",
       "              323,    448,   1550,  12183,  16539,     11,    279,  18927,    315,\n",
       "             1660,  56496,    311,  89077,    374,    220,     20,     23,  14360,\n",
       "             1752,  31695,     76,  40681,     11,    279,  18927,    315,   1550,\n",
       "            12183,  16539,    374,    220,     16,     24,  14360,   1752,  56657,\n",
       "               11,    279,  18927,    315,   1550,  12183,  16539,    374,    220,\n",
       "               22,     16,  14360,   1752,  56657,     11,   1035,    432,    387,\n",
       "              803,   4363,    311,   1490,   1660,  56496,    311,  89077,    421,\n",
       "              279,   1697,   1030,   1012,    264,  31695,     76,  10451,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151646,  51057,    264,    656,  95224,     11,  58059,\n",
       "             1879,    448,   1172,    279,   2701,   4682,     11,    323,   2041,\n",
       "              894,    650,  36591,   9363,    476,  58457,  11871,     25,    619,\n",
       "               88,   4554,    702,    264,   2118,   2456,    389,    379,   7564,\n",
       "               13,    809,   7564,    702,    264,   2118,   2456,    389,    595,\n",
       "             1126,     87,     13,   1752,   1846,    879,    525,    537,    502,\n",
       "               88,   4554,     11,    279,  18927,    315,    595,   1126,     87,\n",
       "              374,    220,     24,     19,  14360,   1752,   1846,    879,    525,\n",
       "              502,     88,   4554,     11,    279,  18927,    315,    595,   1126,\n",
       "               87,    374,    220,     24,     21,  14360,   4841,    502,     88,\n",
       "             4554,   5263,    279,   6012,    315,    595,   1126,     87,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,    656,\n",
       "            95224,     11,  58059,   1879,    448,   1172,    279,   2701,   4682,\n",
       "               11,    323,   2041,    894,    650,  36591,   9363,    476,  58457,\n",
       "            11871,     25,    393,    327,     84,    702,    264,   2118,   2456,\n",
       "              389,    435,   3101,     89,     13,    472,     86,    706,    702,\n",
       "              264,   2118,   2456,    389,    435,   3101,     89,     13,    576,\n",
       "             8084,  18927,    315,    281,    327,     84,    374,    220,     19,\n",
       "               22,  14360,   1752,   1846,    879,    525,    537,    281,    327,\n",
       "               84,     11,    279,  18927,    315,    435,   3101,     89,    374,\n",
       "              220,     22,     19,  14360,   1752,   1846,    879,    525,    281,\n",
       "              327,     84,     11,    279,  18927,    315,    435,   3101,     89,\n",
       "              374,    220,     21,     19,  14360,   2160,    435,   3101,     89,\n",
       "             2686,   4363,   1091,    537,    435,   3101,     89,   8084,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151646,  51057,    264,    656,  95224,     11,  58059,   1879,\n",
       "              448,   1172,    279,   2701,   4682,     11,    323,   2041,    894,\n",
       "              650,  36591,   9363,    476,  58457,  11871,     25,  54253,    702,\n",
       "              264,   2118,   2456,    389,   8149,    323,   3349,     13,  77378,\n",
       "              817,  45223,    702,    264,   2118,   2456,    389,   8149,     13,\n",
       "            29809,    702,    264,   2118,   2456,    389,   3349,     13,  54253,\n",
       "              374,    650,   5481,   2771,     13,    576,   8084,  18927,    315,\n",
       "             7172,   8149,    374,    220,     19,     17,  14360,    576,  18927,\n",
       "              315,  11046,   8149,    323,   7172,   3349,    374,    220,     19,\n",
       "               17,  14360,    576,  18927,    315,   7172,   8149,    323,   7172,\n",
       "             3349,    374,    220,     17,     17,  14360,   2160,    279,   6012,\n",
       "              315,   7172,   3349,   9155,    979,  44971,   7172,   8149,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,    656,\n",
       "            95224,     11,  58059,   1879,    448,   1172,    279,   2701,   4682,\n",
       "               11,    323,   2041,    894,    650,  36591,   9363,    476,  58457,\n",
       "            11871,     25,   4968,     12,  16495,    702,    264,   2118,   2456,\n",
       "              389,  46065,    323,   8457,     13,  58688,   2554,    702,    264,\n",
       "             2118,   2456,    389,   8457,     13,    576,   8084,  18927,    315,\n",
       "             3709,    279,  24467,    374,    220,     18,     24,  14360,    576,\n",
       "            18927,    315,  24467,  43151,    323,  41286,    504,    279,   8457,\n",
       "              374,    220,     17,     15,  14360,    576,  18927,    315,   3709,\n",
       "              279,  24467,    323,  41286,    504,    279,   8457,    374,    220,\n",
       "               18,     16,  14360,   2160,    279,   6012,    315,  41286,    504,\n",
       "              279,   8457,   8131,    979,  44971,   3709,    279,  24467,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['no', 'yes', 'no', 'yes', 'yes'],\n",
       "  'metadata': {'id': [14640, 23036, 24737, 3504, 8750],\n",
       "   'reasoning': ['Let V1 = gender; X = smoking; V3 = tar deposit; Y = peanut allergy.\\nV1->X,X->V3,V1->Y,V3->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\n\\\\sum_{V3=v}P(Y=1|X=1,V3=v)*[P(V3=v|X=1)-P(V3=v|X=0)]\\nP(Y=1 | X=0, V3=0) = 0.25\\nP(Y=1 | X=0, V3=1) = 0.57\\nP(Y=1 | X=1, V3=0) = 0.26\\nP(Y=1 | X=1, V3=1) = 0.58\\nP(V3=1 | X=0) = 0.19\\nP(V3=1 | X=1) = 0.71\\n0.58 * (0.71 - 0.19) + 0.57 * (0.29 - 0.81) = 0.17\\n0.17 > 0',\n",
       "    'Let X = jyka; V2 = yupt; Y = kwox.\\nX->V2,V2->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.94\\nP(Y=1 | X=1) = 0.96\\n0.96 - 0.94 = 0.01\\n0.01 > 0',\n",
       "    'Let V2 = hwax; X = pexu; Y = rukz.\\nX->Y,V2->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.47\\nP(Y=1 | X=0) = 0.74\\nP(Y=1 | X=1) = 0.64\\n0.47*0.64 - 0.53*0.74 = 0.69\\n0.69 > 0',\n",
       "    'Let V2 = yield per acre; V1 = demand; X = supply; Y = price.\\nV1->X,V2->X,V1->Y,X->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.42\\nP(Y=1, X=0=1) = 0.42\\nP(Y=1, X=1=1) = 0.22\\n0.22/0.42 - 0.42/0.58 = -0.22\\n-0.22 < 0',\n",
       "    'Let V1 = pre-conditions; X = vaccination; Y = disease.\\nV1->X,V1->Y,X->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.39\\nP(Y=1, X=0=1) = 0.20\\nP(Y=1, X=1=1) = 0.31\\n0.31/0.39 - 0.20/0.61 = 0.47\\n0.47 > 0'],\n",
       "   'rung': [3, 2, 1, 1, 1],\n",
       "   'query_type': ['ett', 'ate', 'marginal', 'correlation', 'correlation'],\n",
       "   'graph_id': ['frontdoor', 'chain', 'fork', 'IV', 'confounding'],\n",
       "   'story_id': ['smoking_frontdoor',\n",
       "    'nonsense5',\n",
       "    'nonsense8',\n",
       "    'price',\n",
       "    'simpson_vaccine'],\n",
       "   'question_property': ['anticommonsense',\n",
       "    'nonsense',\n",
       "    'nonsense',\n",
       "    'easy',\n",
       "    'hard'],\n",
       "   'formal_form': ['E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'E[Y | do(X = 1)] - E[Y | do(X = 0)]',\n",
       "    'P(Y)',\n",
       "    'P(Y | X)',\n",
       "    'P(Y | X)']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151646,  51057,    264,    656,  95224,     11,\n",
       "            58059,   1879,    448,   1172,    279,   2701,   4682,     11,    323,\n",
       "             2041,    894,    650,  36591,   9363,    476,  58457,  11871,     25,\n",
       "             1230,   5481,   2771,   2335,    795,    388,    702,    264,   2118,\n",
       "             2456,    389,   6731,   2188,    323,  16107,     13,   1298,  91728,\n",
       "              311,    264,   7770,    702,    264,   2118,   2456,    389,   6731,\n",
       "             2188,     13,  11668,   2188,    702,    264,   2118,   2456,    389,\n",
       "            16107,     13,   1230,   5481,   2771,   2335,    795,    388,    374,\n",
       "              650,   5481,   2771,     13,   6730,    220,     16,     25,   1205,\n",
       "             1401,   5961,    518,   1246,   6731,   2188,  96203,    448,  16107,\n",
       "              304,   4586,     13,   6730,    220,     17,     25,   1205,   1401,\n",
       "              518,    419,  25588,   1142,    553,   1142,   4092,    311,    650,\n",
       "             5481,   2771,   2335,    795,    388,     13,   2014,   3535,   1246,\n",
       "             6731,   2188,  21501,  16107,     11,    374,    432,    803,   4396,\n",
       "              311,    990,    279,   6730,    220,     16,   1091,   6730,    220,\n",
       "               17,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151646,  51057,    264,    656,  95224,\n",
       "               11,  58059,   1879,    448,   1172,    279,   2701,   4682,     11,\n",
       "              323,   2041,    894,    650,  36591,   9363,    476,  58457,  11871,\n",
       "               25,  60616,    702,    264,   2118,   2456,    389,  32551,     13,\n",
       "            54507,    702,    264,   2118,   2456,    389,  32551,     13,   6730,\n",
       "              220,     16,     25,   1205,   1401,    518,   1246,  11094,  96203,\n",
       "              448,  10772,   1142,    553,   1142,   4092,    311,  32551,     13,\n",
       "             6730,    220,     17,     25,   1205,   1401,   5961,    518,   1246,\n",
       "            11094,  96203,    448,  10772,    304,   4586,     13,   2014,   3535,\n",
       "             1246,  11094,  21501,  10772,     11,    374,    432,    803,   4396,\n",
       "              311,    990,    279,   6730,    220,     16,   1091,   6730,    220,\n",
       "               17,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151646,  51057,    264,    656,  95224,\n",
       "               11,  58059,   1879,    448,   1172,    279,   2701,   4682,     11,\n",
       "              323,   2041,    894,    650,  36591,   9363,    476,  58457,  11871,\n",
       "               25,    350,   3172,     85,    702,    264,   2118,   2456,    389,\n",
       "              259,   3172,     86,    323,  82790,   3334,     13,   1599,     68,\n",
       "             3334,    702,    264,   2118,   2456,    389,    342,  18348,     79,\n",
       "               13,    350,   3172,     86,    702,    264,   2118,   2456,    389,\n",
       "              342,  18348,     79,     13,    576,   8084,  18927,    315,  82790,\n",
       "             3334,    374,    220,     21,     19,  14360,   1752,   1846,    879,\n",
       "              525,    537,  82790,   3334,     11,    279,  18927,    315,    342,\n",
       "            18348,     79,    374,    220,     19,     17,  14360,   1752,   1846,\n",
       "              879,    525,  82790,   3334,     11,    279,  18927,    315,    342,\n",
       "            18348,     79,    374,    220,     17,     18,  14360,   2160,    342,\n",
       "            18348,     79,   2686,   4363,   1091,    537,    342,  18348,     79,\n",
       "             8084,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151646,  51057,    264,    656,  95224,     11,  58059,\n",
       "             1879,    448,   1172,    279,   2701,   4682,     11,    323,   2041,\n",
       "              894,    650,  36591,   9363,    476,  58457,  11871,     25,    809,\n",
       "              316,     87,    702,    264,   2118,   2456,    389,  56572,    295,\n",
       "               13,    479,     86,    295,    702,    264,   2118,   2456,    389,\n",
       "              856,   5120,     84,     13,   6730,    220,     16,     25,   1205,\n",
       "             1401,    518,   1246,    379,    316,     87,  96203,    448,    856,\n",
       "             5120,     84,   1142,    553,   1142,   4092,    311,  56572,    295,\n",
       "               13,   6730,    220,     17,     25,   1205,   1401,   5961,    518,\n",
       "             1246,    379,    316,     87,  96203,    448,    856,   5120,     84,\n",
       "              304,   4586,     13,   2014,   3535,   1246,    379,    316,     87,\n",
       "            21501,    856,   5120,     84,     11,    374,    432,    803,   4396,\n",
       "              311,    990,    279,   6730,    220,     16,   1091,   6730,    220,\n",
       "               17,     30],\n",
       "          [151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  64815,    702,    264,\n",
       "             2118,   2456,    389,   5025,   2639,     13,   7577,    377,    642,\n",
       "              702,    264,   2118,   2456,    389,   5025,   2639,     13,    576,\n",
       "             8084,  18927,    315,  44872,    374,    220,     19,     22,  14360,\n",
       "             1752,   1251,    448,   3076,  43808,    323,   3175,   1251,     11,\n",
       "              279,  18927,    315,   3457,    377,    642,    374,    220,     17,\n",
       "               15,  14360,   1752,   1251,    448,   3076,  43808,    323,    304,\n",
       "              264,   5025,     11,    279,  18927,    315,   3457,    377,    642,\n",
       "              374,    220,     16,     16,  14360,   1752,   1251,    448,   3093,\n",
       "            43808,    323,   3175,   1251,     11,    279,  18927,    315,   3457,\n",
       "              377,    642,    374,    220,     17,  14360,   1752,   1251,    448,\n",
       "             3093,  43808,    323,    304,    264,   5025,     11,    279,  18927,\n",
       "              315,   3457,    377,    642,    374,    220,     17,     16,  14360,\n",
       "             1416,    582,   1401,    518,   1251,    304,    264,   5025,     11,\n",
       "             1558,    279,   6012,    315,   3457,    377,    642,   5263,    979,\n",
       "            44872,     30]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'yes', 'no', 'yes'],\n",
       "  'metadata': {'id': [6624, 1569, 26425, 26522, 13185],\n",
       "   'reasoning': ['nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let V1 = tijv; V3 = tijw; X = xevo; Y = gyzp.\\nV1->V3,V1->X,X->Y,V3->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.64\\nP(Y=1 | X=0) = 0.42\\nP(Y=1 | X=1) = 0.23\\n0.64*0.23 - 0.36*0.42 = 0.30\\n0.30 > 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let Y = freckles; X = personality; V3 = relationship status.\\nX->V3,Y->V3\\nP(Y = 1 | X = 1, V3 = 1] - P(Y = 1 | V3 = 1)\\nP(Y=1 | X=1, V3=1) - (P(X=1) * P(Y=1 | X=1, V3=1) + P(X=0) * P(Y=1 | X=0, V3=1))\\nP(X=1) = 0.47\\nP(Y=1 | X=0, V3=0) = 0.20\\nP(Y=1 | X=0, V3=1) = 0.11\\nP(Y=1 | X=1, V3=0) = 0.02\\nP(Y=1 | X=1, V3=1) = 0.21\\n0.21 - (0.47*0.21 + 0.53*0.11) = 0.06\\n0.06 > 0'],\n",
       "   'rung': [2, 2, 1, 2, 1],\n",
       "   'query_type': ['backadj', 'backadj', 'marginal', 'backadj', 'exp_away'],\n",
       "   'graph_id': ['IV', 'collision', 'diamondcut', 'chain', 'collision'],\n",
       "   'story_id': ['college_wage',\n",
       "    'celebrity',\n",
       "    'nonsense2',\n",
       "    'nonsense3',\n",
       "    'man_in_relationship'],\n",
       "   'question_property': ['hard',\n",
       "    'easy',\n",
       "    'nonsense',\n",
       "    'nonsense',\n",
       "    'anticommonsense'],\n",
       "   'formal_form': ['[backdoor adjustment set for Y given X]',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    'P(Y)',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    'P(Y = 1 | X = 1, V3 = 1] - P(Y = 1 | V3 = 1)']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,  25293,   2639,     30],\n",
       "          [151643, 151643, 151643,  ...,   3347,  37986,     30],\n",
       "          [151643, 151643, 151643,  ...,  31256,    706,     30],\n",
       "          [151643, 151643, 151643,  ...,  82790,   3334,     30],\n",
       "          [151646,  51057,    264,  ...,   3590,   2639,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'yes', 'yes', 'no'],\n",
       "  'metadata': {'id': [7012, 16786, 26946, 22088, 7620],\n",
       "   'reasoning': ['Let X = gender; V2 = department competitiveness; Y = admission status.\\nX->V2,X->Y,V2->Y\\nE[Y_{X=1, V2=0} - Y_{X=0, V2=0}]\\n\\\\sum_{V2=v} P(V2=v|X=0)*[P(Y=1|X=1,V2=v) - P(Y=1|X=0, V2=v)]\\nP(Y=1 | X=0, V2=0) = 0.26\\nP(Y=1 | X=0, V2=1) = 0.67\\nP(Y=1 | X=1, V2=0) = 0.38\\nP(Y=1 | X=1, V2=1) = 0.63\\nP(V2=1 | X=0) = 0.65\\nP(V2=1 | X=1) = 0.11\\n0.65 * (0.63 - 0.38) + 0.11 * (0.67 - 0.26) = 0.02\\n0.02 > 0',\n",
       "    'Let V2 = treatment assignment; V1 = unobserved confounders; X = having a sister; Y = cholesterol level.\\nV1->X,V2->X,V1->Y,X->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\n[P(Y=1|V2=1)-P(Y=1|V2=0)]/[P(X=1|V2=1)-P(X=1|V2=0)]\\nP(Y=1 | V2=0) = 0.54\\nP(Y=1 | V2=1) = 0.52\\nP(X=1 | V2=0) = 0.79\\nP(X=1 | V2=1) = 0.43\\n(0.52 - 0.54) / (0.43 - 0.79) = 0.05\\n0.05 > 0',\n",
       "    'Let X = jyka; V2 = hwax; Y = lirg.\\nX->V2,V2->Y\\nE[Y_{X=0, V2=1} - Y_{X=0, V2=0}]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.59\\nP(Y=1 | X=1) = 0.66\\n0.66 - 0.59 = 0.08\\n0.08 > 0',\n",
       "    'Let V2 = tijv; X = xevo; Y = gyzp.\\nX->Y,V2->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.88\\nP(Y=1, X=0=1) = 0.04\\nP(Y=1, X=1=1) = 0.73\\n0.73/0.88 - 0.04/0.12 = 0.53\\n0.53 > 0',\n",
       "    \"Let V2 = other unobserved factors; X = parents' intelligence; V3 = parents' social status; Y = child's intelligence.\\nX->V3,V2->V3,X->Y,V2->Y,V3->Y\\nE[Y_{X=0, V3=1} - Y_{X=0, V3=0}]\\n\\\\sum_{V3=v} [\\\\sum_{V2=k} P(Y=1|X=0,V3=v)*[P(V3=v|X=1,V2=k)-P(V3=v|X=0,V2=k)]*P(V2=k)]\\nP(Y=1 | X=0, V3=0) = 0.82\\nP(Y=1 | X=0, V3=1) = 0.46\\nP(Y=1 | X=1, V3=0) = 0.48\\nP(Y=1 | X=1, V3=1) = 0.14\\nP(V3=1 | X=0, V2=0) = 0.63\\nP(V3=1 | X=0, V2=1) = 0.68\\nP(V3=1 | X=1, V2=0) = 0.16\\nP(V3=1 | X=1, V2=1) = 0.30\\nP(V2=1) = 0.53\\n0.53 * 0.46 * (0.30 - 0.16)+ 0.47 * 0.46 * (0.68 - 0.63)= 0.15\\n0.15 > 0\"],\n",
       "   'rung': [3, 2, 3, 1, 3],\n",
       "   'query_type': ['nde', 'ate', 'nie', 'correlation', 'nie'],\n",
       "   'graph_id': ['mediation', 'IV', 'chain', 'fork', 'arrowhead'],\n",
       "   'story_id': ['gender_admission',\n",
       "    'cholesterol',\n",
       "    'nonsense4',\n",
       "    'nonsense2',\n",
       "    'nature_vs_nurture'],\n",
       "   'question_property': ['hard',\n",
       "    'anticommonsense',\n",
       "    'nonsense',\n",
       "    'nonsense',\n",
       "    'hard'],\n",
       "   'formal_form': ['E[Y_{X=1, V2=0} - Y_{X=0, V2=0}]',\n",
       "    'E[Y | do(X = 1)] - E[Y | do(X = 0)]',\n",
       "    'E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]',\n",
       "    'P(Y | X)',\n",
       "    'E[Y_{X=0, V3=1} - Y_{X=0, V3=0}]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151646,  51057,    264,    656,  95224,     11,  58059,\n",
       "             1879,    448,   1172,    279,   2701,   4682,     11,    323,   2041,\n",
       "              894,    650,  36591,   9363,    476,  58457,  11871,     25,  35983,\n",
       "              311,  16191,    702,    264,   2118,   2456,    389,  20956,  14132,\n",
       "              323,   7006,   5456,     13,   7659,   7024,  14132,    702,    264,\n",
       "             2118,   2456,    389,   7006,   5456,     13,   1752,   4143,    879,\n",
       "             4157,  16191,     11,    279,  18927,    315,   1550,   7006,   5456,\n",
       "              374,    220,     17,     21,  14360,   1752,   4143,    879,    646,\n",
       "            16191,     11,    279,  18927,    315,   1550,   7006,   5456,    374,\n",
       "              220,     22,     17,  14360,   1752,   4143,    879,    646,  16191,\n",
       "               11,   1035,    432,    387,    803,   4363,    311,   1490,   1550,\n",
       "             7006,   5456,    421,    279,   5458,   1410,  16191,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151646,  51057,    264,    656,  95224,     11,\n",
       "            58059,   1879,    448,   1172,    279,   2701,   4682,     11,    323,\n",
       "             2041,    894,    650,  36591,   9363,    476,  58457,  11871,     25,\n",
       "            28217,    702,    264,   2118,   2456,    389,  19578,    323,  20622,\n",
       "             9387,     13,  72599,    702,    264,   2118,   2456,    389,  12183,\n",
       "            16539,     13,  23959,  16539,    702,    264,   2118,   2456,    389,\n",
       "            20622,   9387,     13,  28217,    374,    650,   5481,   2771,     13,\n",
       "             1205,   1414,    429,   8778,  11137,  19578,     13,  19578,  11137,\n",
       "             1550,  12183,  16539,     13,   8778,    476,   1550,  12183,  16539,\n",
       "            11137,  20622,   9387,     13,   1205,  13166,    279,   1697,    374,\n",
       "             8778,     13,  18885,    279,   1697,    702,    902,  20622,   9387,\n",
       "              421,  31695,     76,  10746,   4518,    315,  19578,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151646,  51057,    264,    656,  95224,     11,  58059,\n",
       "             1879,    448,   1172,    279,   2701,   4682,     11,    323,   2041,\n",
       "              894,    650,  36591,   9363,    476,  58457,  11871,     25,  29405,\n",
       "            47628,    705,    702,    264,   2118,   2456,    389,  29405,  32011,\n",
       "              311,   2906,     13,  36981,    702,    264,   2118,   2456,    389,\n",
       "            29405,  32011,    311,   2906,     13,   1752,   2849,    979,  29405,\n",
       "            61567,    705,    389,    882,     11,    279,  18927,    315,  32011,\n",
       "              311,   2906,    389,    882,    374,    220,     23,     21,  14360,\n",
       "             1752,   2849,    979,  29405,  61567,    705,   3309,     11,    279,\n",
       "            18927,    315,  32011,    311,   2906,    389,    882,    374,    220,\n",
       "               19,     22,  14360,   4841,  47628,    705,   3309,   5263,    279,\n",
       "             6012,    315,  32011,    311,   2906,    389,    882,     30],\n",
       "          [151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  28217,    702,    264,\n",
       "             2118,   2456,    389,  19578,    323,   3457,    377,    642,     13,\n",
       "            72599,    702,    264,   2118,   2456,    389,  12183,  16539,     13,\n",
       "            23959,  16539,    702,    264,   2118,   2456,    389,   3457,    377,\n",
       "              642,     13,  28217,    374,    650,   5481,   2771,     13,   6730,\n",
       "              220,     16,     25,   1205,   1401,    518,   1246,  19578,  96203,\n",
       "              448,   3457,    377,    642,   1142,    553,   1142,   4092,    311,\n",
       "             9825,     13,   6730,    220,     17,     25,   1205,   1401,   5961,\n",
       "              518,   1246,  19578,  96203,    448,   3457,    377,    642,    304,\n",
       "             4586,     13,   2014,   3535,   1246,  19578,  21501,   3457,    377,\n",
       "              642,     11,    374,    432,    803,   4396,    311,    990,    279,\n",
       "             6730,    220,     16,   1091,   6730,    220,     17,     30],\n",
       "          [151643, 151646,  51057,    264,    656,  95224,     11,  58059,   1879,\n",
       "              448,   1172,    279,   2701,   4682,     11,    323,   2041,    894,\n",
       "              650,  36591,   9363,    476,  58457,  11871,     25,    350,   3172,\n",
       "               85,    702,    264,   2118,   2456,    389,    259,   3172,     86,\n",
       "              323,  82790,   3334,     13,   1599,     68,   3334,    702,    264,\n",
       "             2118,   2456,    389,    342,  18348,     79,     13,    350,   3172,\n",
       "               86,    702,    264,   2118,   2456,    389,    342,  18348,     79,\n",
       "               13,    576,   8084,  18927,    315,  82790,   3334,    374,    220,\n",
       "               21,     16,  14360,   1752,   1846,    879,    525,    537,  82790,\n",
       "             3334,     11,    279,  18927,    315,    342,  18348,     79,    374,\n",
       "              220,     17,     24,  14360,   1752,   1846,    879,    525,  82790,\n",
       "             3334,     11,    279,  18927,    315,    342,  18348,     79,    374,\n",
       "              220,     19,     16,  14360,   2160,    342,  18348,     79,   2686,\n",
       "             4363,   1091,    537,    342,  18348,     79,   8084,     30]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1],\n",
       "          [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'no', 'yes', 'yes'],\n",
       "  'metadata': {'id': [12416, 10225, 891, 19354, 26418],\n",
       "   'reasoning': ['Let X = ability to swim; V2 = studying habit; Y = exam score.\\nX->V2,X->Y,V2->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.26\\nP(Y=1 | X=1) = 0.72\\n0.72 - 0.26 = 0.46\\n0.46 > 0',\n",
       "    'Let V1 = gender; X = smoking; V3 = tar deposit; Y = lung cancer.\\nV1->X,X->V3,V1->Y,V3->Y\\nY_{X=0} = 0 | V1=1\\nSolve for Y, given the evidence and the action\\nV1 = 1\\nX = V1\\nV3 = X\\nY = V1 or V3\\nY = 1 = 1 or 0\\n0',\n",
       "    'Let V2 = traffic; X = Alice waking up; Y = Alice arriving to school.\\nX->Y,V2->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\nP(Y|X)\\nP(Y=1 | X=0) = 0.86\\nP(Y=1 | X=1) = 0.47\\n0.47 - 0.86 = -0.39\\n-0.39 < 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let V1 = tijv; V3 = tijw; X = xevo; Y = gyzp.\\nV1->V3,V1->X,X->Y,V3->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.61\\nP(Y=1 | X=0) = 0.29\\nP(Y=1 | X=1) = 0.41\\n0.61*0.41 - 0.39*0.29 = 0.36\\n0.36 > 0'],\n",
       "   'rung': [3, 3, 2, 2, 1],\n",
       "   'query_type': ['ett', 'det-counterfactual', 'ate', 'backadj', 'marginal'],\n",
       "   'graph_id': ['mediation', 'frontdoor', 'fork', 'frontdoor', 'diamondcut'],\n",
       "   'story_id': ['encouagement_program',\n",
       "    'smoking_frontdoor',\n",
       "    'getting_late',\n",
       "    'smoking_frontdoor',\n",
       "    'nonsense2'],\n",
       "   'question_property': ['anticommonsense',\n",
       "    'commonsense',\n",
       "    'easy',\n",
       "    'anticommonsense',\n",
       "    'nonsense'],\n",
       "   'formal_form': ['E[Y_{X = 1} - Y_{X = 0} | X = 1]',\n",
       "    'Y_{X=0} = 0 | V1=1',\n",
       "    'E[Y | do(X = 1)] - E[Y | do(X = 0)]',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    'P(Y)']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643,  ...,    454,     71,     30],\n",
       "          [151643, 151643, 151643,  ...,    220,     17,     30],\n",
       "          [151643, 151643, 151643,  ...,   4240,   3015,     30],\n",
       "          [151643, 151643, 151643,  ...,    220,     17,     30],\n",
       "          [151646,  51057,    264,  ...,    941,     80,     30]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]])},\n",
       "  'labels': ['yes', 'no', 'yes', 'no', 'yes'],\n",
       "  'metadata': {'id': [29179, 26679, 19819, 5600, 20981],\n",
       "   'reasoning': ['Let X = zuph; V2 = jyka; Y = glimx.\\nX->V2,V2->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.98\\nP(Y=1, X=0=1) = 0.00\\nP(Y=1, X=1=1) = 0.10\\n0.10/0.98 - 0.00/0.02 = 0.01\\n0.01 > 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let V2 = water company; V1 = poverty; X = water quality; Y = freckles.\\nV1->X,V2->X,V1->Y,X->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.22\\nP(Y=1, X=0=1) = 0.44\\nP(Y=1, X=1=1) = 0.17\\n0.17/0.22 - 0.44/0.78 = 0.22\\n0.22 > 0',\n",
       "    'nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan',\n",
       "    'Let V1 = swoy; X = rixq; V3 = zuph; Y = xevu.\\nV1->X,X->V3,V1->Y,V3->Y\\nE[Y_{X = 1} - Y_{X = 0} | X = 1]\\n\\\\sum_{V3=v}P(Y=1|X=1,V3=v)*[P(V3=v|X=1)-P(V3=v|X=0)]\\nP(Y=1 | X=0, V3=0) = 0.40\\nP(Y=1 | X=0, V3=1) = 0.53\\nP(Y=1 | X=1, V3=0) = 0.72\\nP(Y=1 | X=1, V3=1) = 0.35\\nP(V3=1 | X=0) = 0.36\\nP(V3=1 | X=1) = 0.38\\n0.35 * (0.38 - 0.36) + 0.53 * (0.62 - 0.64) = -0.01\\n-0.01 < 0'],\n",
       "   'rung': [1, 2, 1, 2, 3],\n",
       "   'query_type': ['correlation', 'backadj', 'correlation', 'backadj', 'ett'],\n",
       "   'graph_id': ['chain', 'frontdoor', 'IV', 'fork', 'frontdoor'],\n",
       "   'story_id': ['nonsense9',\n",
       "    'nonsense3',\n",
       "    'water_cholera',\n",
       "    'getting_late',\n",
       "    'nonsense0'],\n",
       "   'question_property': ['nonsense',\n",
       "    'nonsense',\n",
       "    'anticommonsense',\n",
       "    'hard',\n",
       "    'nonsense'],\n",
       "   'formal_form': ['P(Y | X)',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    'P(Y | X)',\n",
       "    '[backdoor adjustment set for Y given X]',\n",
       "    'E[Y_{X = 1} - Y_{X = 0} | X = 1]']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,    656,\n",
       "            95224,     11,  58059,   1879,    448,   1172,    279,   2701,   4682,\n",
       "               11,    323,   2041,    894,    650,  36591,   9363,    476,  58457,\n",
       "            11871,     25,  64815,    702,    264,   2118,   2456,    389,   5025,\n",
       "             2639,     13,  60616,    702,    264,   2118,   2456,    389,   5025,\n",
       "             2639,     13,   1752,   1251,    304,    264,   5025,     11,    279,\n",
       "            25588,   1948,  44872,    323,  18879,  11094,    374,    481,     15,\n",
       "               13,     15,     17,     13,   1416,    582,   1401,    518,   1251,\n",
       "              304,    264,   5025,     11,   1558,    432,   3076,    429,  44872,\n",
       "             1558,    537,   7802,  18879,  11094,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,    656,\n",
       "            95224,     11,  58059,   1879,    448,   1172,    279,   2701,   4682,\n",
       "               11,    323,   2041,    894,    650,  36591,   9363,    476,  58457,\n",
       "            11871,     25,    393,    327,     84,    702,    264,   2118,   2456,\n",
       "              389,    595,   8703,    323,  31256,    706,     13,    472,     86,\n",
       "              706,    702,    264,   2118,   2456,    389,    435,   3101,     89,\n",
       "               13,    730,   8703,    702,    264,   2118,   2456,    389,    435,\n",
       "             3101,     89,     13,    576,   8084,  18927,    315,    281,    327,\n",
       "               84,    374,    220,     21,     22,  14360,    576,  18927,    315,\n",
       "              537,    281,    327,     84,    323,    435,   3101,     89,    374,\n",
       "              220,     16,     18,  14360,    576,  18927,    315,    281,    327,\n",
       "               84,    323,    435,   3101,     89,    374,    220,     17,     17,\n",
       "            14360,   2160,    279,   6012,    315,    435,   3101,     89,   9155,\n",
       "              979,  44971,    281,    327,     84,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,\n",
       "              264,    656,  95224,     11,  58059,   1879,    448,   1172,    279,\n",
       "             2701,   4682,     11,    323,   2041,    894,    650,  36591,   9363,\n",
       "              476,  58457,  11871,     25,    619,     88,   4554,    702,    264,\n",
       "             2118,   2456,    389,    379,   7564,     13,    809,   7564,    702,\n",
       "              264,   2118,   2456,    389,    595,   1126,     87,     13,   1752,\n",
       "             1846,    879,    525,    537,    502,     88,   4554,     11,    279,\n",
       "            18927,    315,    595,   1126,     87,    374,    220,     18,     17,\n",
       "            14360,   1752,   1846,    879,    525,    502,     88,   4554,     11,\n",
       "              279,  18927,    315,    595,   1126,     87,    374,    220,     18,\n",
       "               19,  14360,  12553,    502,     88,   4554,  39546,   7802,    595,\n",
       "             1126,     87,   1526,    379,   7564,     30],\n",
       "          [151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,  28217,    702,    264,\n",
       "             2118,   2456,    389,  19578,    323,  20622,   9387,     13,  72599,\n",
       "              702,    264,   2118,   2456,    389,  12183,  16539,     13,  23959,\n",
       "            16539,    702,    264,   2118,   2456,    389,  20622,   9387,     13,\n",
       "            28217,    374,    650,   5481,   2771,     13,   1752,  31695,     76,\n",
       "            40681,     11,    279,  18927,    315,   1550,  12183,  16539,    374,\n",
       "              220,     16,     21,  14360,   1752,  56657,     11,    279,  18927,\n",
       "              315,   1550,  12183,  16539,    374,    220,     24,     18,  14360,\n",
       "             1752,  31695,     76,  40681,    323,    448,    902,  12183,  16539,\n",
       "               11,    279,  18927,    315,  20622,   9387,    374,    220,     17,\n",
       "               18,  14360,   1752,  31695,     76,  40681,    323,    448,   1550,\n",
       "            12183,  16539,     11,    279,  18927,    315,  20622,   9387,    374,\n",
       "              220,     20,     15,  14360,   1752,  56657,    323,    448,    902,\n",
       "            12183,  16539,     11,    279,  18927,    315,  20622,   9387,    374,\n",
       "              220,     16,     17,  14360,   1752,  56657,    323,    448,   1550,\n",
       "            12183,  16539,     11,    279,  18927,    315,  20622,   9387,    374,\n",
       "              220,     18,     23,  14360,    576,   8084,  18927,    315,  19578,\n",
       "              374,    220,     20,     15,  14360,  12553,  19578,  47191,   7802,\n",
       "            20622,   9387,   1526,  12183,  16539,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,\n",
       "            51057,    264,    656,  95224,     11,  58059,   1879,    448,   1172,\n",
       "              279,   2701,   4682,     11,    323,   2041,    894,    650,  36591,\n",
       "             9363,    476,  58457,  11871,     25,  28217,    702,    264,   2118,\n",
       "             2456,    389,  19578,    323,   3457,    377,    642,     13,  72599,\n",
       "              702,    264,   2118,   2456,    389,  12183,  16539,     13,  23959,\n",
       "            16539,    702,    264,   2118,   2456,    389,   3457,    377,    642,\n",
       "               13,  28217,    374,    650,   5481,   2771,     13,    576,   8084,\n",
       "            18927,    315,  19578,    374,    220,     22,     20,  14360,   1752,\n",
       "            31695,     76,  40681,     11,    279,  18927,    315,   3457,    377,\n",
       "              642,    374,    220,     17,     21,  14360,   1752,  56657,     11,\n",
       "              279,  18927,    315,   3457,    377,    642,    374,    220,     20,\n",
       "               23,  14360,   2160,   3457,    377,    642,    803,   4363,   1091,\n",
       "              902,   3457,    377,    642,   8084,     30]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['yes', 'yes', 'yes', 'no', 'yes'],\n",
       "  'metadata': {'id': [7576, 29032, 27371, 4227, 19428],\n",
       "   'reasoning': ['Let Y = appearance; X = personality; V3 = relationship status.\\nX->V3,Y->V3\\nE[Y = 1 | do(X = 1), V3 = 1] - E[Y = 1 | do(X = 0), V3 = 1]\\nX and Y do not affect each other.\\n\\n0\\nyes',\n",
       "    'Let X = pexu; V3 = kraz; V2 = hwax; Y = rukz.\\nX->V3,X->V2,V2->Y,V3->Y\\nP(Y | X)\\nP(X = 1, Y = 1)/P(X = 1) - P(X = 0, Y = 1)/P(X = 0)\\nP(X=1=1) = 0.67\\nP(Y=1, X=0=1) = 0.13\\nP(Y=1, X=1=1) = 0.22\\n0.22/0.67 - 0.13/0.33 = -0.05\\n-0.05 < 0',\n",
       "    'Let X = jyka; V2 = yupt; Y = kwox.\\nX->V2,V2->Y\\nE[Y_{X=0, V2=1} - Y_{X=0, V2=0}]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.32\\nP(Y=1 | X=1) = 0.34\\n0.34 - 0.32 = 0.03\\n0.03 > 0',\n",
       "    'Let V1 = gender; X = smoking; V3 = tar deposit; Y = lung cancer.\\nV1->X,X->V3,V1->Y,V3->Y\\nE[Y_{X=0, V3=1} - Y_{X=0, V3=0}]\\n\\\\sum_{V3 = v} [P(V3 = v|X = 1) - P(V3 = v|X = 0)] * [\\\\sum_{X = h} P(Y = 1|X = h,V3 = v)*P(X = h)]\\nP(V3=1 | X=0) = 0.16\\nP(V3=1 | X=1) = 0.93\\nP(Y=1 | X=0, V3=0) = 0.23\\nP(Y=1 | X=0, V3=1) = 0.50\\nP(Y=1 | X=1, V3=0) = 0.12\\nP(Y=1 | X=1, V3=1) = 0.38\\nP(X=1) = 0.50\\n0.93 - 0.16 * (0.38 * 0.50 + 0.50 * 0.50)= 0.20\\n0.20 > 0',\n",
       "    'Let V1 = gender; X = smoking; V3 = tar deposit; Y = freckles.\\nV1->X,X->V3,V1->Y,V3->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.75\\nP(Y=1 | X=0) = 0.26\\nP(Y=1 | X=1) = 0.58\\n0.75*0.58 - 0.25*0.26 = 0.51\\n0.51 > 0'],\n",
       "   'rung': [2, 1, 3, 3, 1],\n",
       "   'query_type': ['collider_bias', 'correlation', 'nie', 'nie', 'marginal'],\n",
       "   'graph_id': ['collision', 'diamond', 'chain', 'frontdoor', 'frontdoor'],\n",
       "   'story_id': ['man_in_relationship',\n",
       "    'nonsense8',\n",
       "    'nonsense5',\n",
       "    'smoking_frontdoor',\n",
       "    'smoking_frontdoor'],\n",
       "   'question_property': ['hard',\n",
       "    'nonsense',\n",
       "    'nonsense',\n",
       "    'easy',\n",
       "    'anticommonsense'],\n",
       "   'formal_form': ['E[Y = 1 | do(X = 1), V3 = 1] - E[Y = 1 | do(X = 0), V3 = 1]',\n",
       "    'P(Y | X)',\n",
       "    'E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]',\n",
       "    'E[Y_{X=0, V3=1} - Y_{X=0, V3=0}]',\n",
       "    'P(Y)']}},\n",
       " {'tokenized_prompts': {'input_ids': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151646,  51057,    264,    656,  95224,\n",
       "               11,  58059,   1879,    448,   1172,    279,   2701,   4682,     11,\n",
       "              323,   2041,    894,    650,  36591,   9363,    476,  58457,  11871,\n",
       "               25,  28217,    702,    264,   2118,   2456,    389,   9292,  76551,\n",
       "              323,  25293,   2639,     13,   5887,  76551,    702,    264,   2118,\n",
       "             2456,    389,  25293,   2639,     13,   1752,   7775,    879,    525,\n",
       "              537,   8593,     11,    279,  18927,    315,  25293,  25505,    374,\n",
       "              220,     18,     22,  14360,   1752,   7775,    879,    525,   8593,\n",
       "               11,    279,  18927,    315,  25293,  25505,    374,    220,     16,\n",
       "               23,  14360,   4841,   8593,   9825,   5263,    279,   6012,    315,\n",
       "            25293,  25505,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,\n",
       "              264,    656,  95224,     11,  58059,   1879,    448,   1172,    279,\n",
       "             2701,   4682,     11,    323,   2041,    894,    650,  36591,   9363,\n",
       "              476,  58457,  11871,     25,    393,    327,     84,    702,    264,\n",
       "             2118,   2456,    389,    595,   8703,    323,  31256,    706,     13,\n",
       "              472,     86,    706,    702,    264,   2118,   2456,    389,    435,\n",
       "             3101,     89,     13,    730,   8703,    702,    264,   2118,   2456,\n",
       "              389,    435,   3101,     89,     13,   1205,   1414,    429,    281,\n",
       "              327,     84,  11137,  31256,    706,    323,    537,    595,   8703,\n",
       "               13,  31256,    706,    476,    595,   8703,  11137,    435,   3101,\n",
       "               89,     13,  18885,    458,   3842,    374,    537,    435,   3101,\n",
       "               89,    421,    281,    327,     84,   4518,    315,    537,    281,\n",
       "              327,     84,     30],\n",
       "          [151646,  51057,    264,    656,  95224,     11,  58059,   1879,    448,\n",
       "             1172,    279,   2701,   4682,     11,    323,   2041,    894,    650,\n",
       "            36591,   9363,    476,  58457,  11871,     25,   6867,   2978,  19578,\n",
       "             2639,    702,    264,   2118,   2456,    389,  30283,    594,   7194,\n",
       "             4680,    323,   3457,    377,    642,     13,   6267,   2971,    702,\n",
       "              264,   2118,   2456,    389,  30283,    594,   7194,   4680,    323,\n",
       "             3457,    377,    642,     13,  82388,    594,   7194,   4680,    702,\n",
       "              264,   2118,   2456,    389,   3457,    377,    642,     13,   6267,\n",
       "             2971,    374,    650,   5481,   2771,     13,    576,   8084,  18927,\n",
       "              315,  19578,   6554,    374,    220,     16,     15,  14360,   1752,\n",
       "            41434,    448,  31695,     76,  10746,  26600,     11,    279,  18927,\n",
       "              315,   3457,    377,    642,    374,    220,     19,     24,  14360,\n",
       "             1752,  41434,    448,  19578,  26600,     11,    279,  18927,    315,\n",
       "             3457,    377,    642,    374,    220,     21,     16,  14360,   2160,\n",
       "             3457,    377,    642,    803,   4363,   1091,    902,   3457,    377,\n",
       "              642,   8084,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,  51057,\n",
       "              264,    656,  95224,     11,  58059,   1879,    448,   1172,    279,\n",
       "             2701,   4682,     11,    323,   2041,    894,    650,  36591,   9363,\n",
       "              476,  58457,  11871,     25,  58688,   2554,   2639,    702,    264,\n",
       "             2118,   2456,    389,  46065,  12720,    323,   3709,   2613,     79,\n",
       "             5131,     13,  24515,   2613,     79,   5131,    702,    264,   2118,\n",
       "             2456,    389,   2613,     79,   5131,  19661,     13,  58688,   2554,\n",
       "            12720,    702,    264,   2118,   2456,    389,   2613,     79,   5131,\n",
       "            19661,     13,   1752,    650,     85,   4475,  15479,   7775,     11,\n",
       "              279,  18927,    315,   2613,     79,   5131,  19661,    374,    220,\n",
       "               20,     21,  14360,   1752,  69458,   7775,     11,    279,  18927,\n",
       "              315,   2613,     79,   5131,  19661,    374,    220,     19,     24,\n",
       "            14360,   4841,  46065,  18472,    279,   6012,    315,   2613,     79,\n",
       "             5131,  19661,     30],\n",
       "          [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "           151643, 151643, 151643, 151643, 151643, 151646,  51057,    264,    656,\n",
       "            95224,     11,  58059,   1879,    448,   1172,    279,   2701,   4682,\n",
       "               11,    323,   2041,    894,    650,  36591,   9363,    476,  58457,\n",
       "            11871,     25,    730,   1126,     87,    702,    264,   2118,   2456,\n",
       "              389,    595,   1126,     89,     13,    328,   1126,     80,    702,\n",
       "              264,   2118,   2456,    389,    595,   1126,     89,     13,   1205,\n",
       "             1414,    429,    595,   1126,     87,    323,  33127,     80,  11137,\n",
       "              595,   1126,     89,     13,   1205,  13166,    458,   3842,    374,\n",
       "            33127,     80,     13,  18885,    458,   3842,    374,    595,   1126,\n",
       "               89,    421,    537,    595,   1126,     87,   4518,    315,    595,\n",
       "             1126,     87,     30]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       "  'labels': ['no', 'no', 'yes', 'yes', 'no'],\n",
       "  'metadata': {'id': [6966, 30847, 14481, 1268, 29868],\n",
       "   'reasoning': ['Let X = gender; V2 = department competitiveness; Y = admission status.\\nX->V2,X->Y,V2->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.37\\nP(Y=1 | X=1) = 0.18\\n0.18 - 0.37 = -0.20\\n-0.20 < 0',\n",
       "    'Let X = pexu; V3 = kraz; V2 = hwax; Y = rukz.\\nX->V3,X->V2,V2->Y,V3->Y\\nY_{X=1} = 0 | \\nSolve for Y, given the evidence and the action\\nV2 = X\\nV3 = not V2\\nY = V2 or V3\\nY = [1] = 1 or 0\\n0',\n",
       "    \"Let V2 = health condition; X = maternal smoking status; V3 = infant's birth weight; Y = freckles.\\nX->V3,V2->V3,X->Y,V2->Y,V3->Y\\nP(Y)\\nP(Y | X=1)*P(X=1) + P(Y | X=0)*P(X=0)\\nP(X=1) = 0.10\\nP(Y=1 | X=0) = 0.49\\nP(Y=1 | X=1) = 0.61\\n0.10*0.61 - 0.90*0.49 = 0.51\\n0.51 > 0\",\n",
       "    'Let X = vaccination status; V3 = vaccination reaction; V2 = getting smallpox; Y = smallpox survival.\\nX->V3,X->V2,V2->Y,V3->Y\\nE[Y | do(X = 1)] - E[Y | do(X = 0)]\\nP(Y=1|X=1) - P(Y=1|X=0)\\nP(Y=1 | X=0) = 0.56\\nP(Y=1 | X=1) = 0.49\\n0.49 - 0.56 = -0.07\\n-0.07 < 0',\n",
       "    'Let V2 = swoq; X = kwox; Y = kwoz.\\nX->Y,V2->Y\\nY_{X=0} = 1 | V2=1\\nSolve for Y, given the evidence and the action\\nV2 = 1\\nY = X and V2\\nY = 0 = 0 and 1\\n0'],\n",
       "   'rung': [2, 3, 1, 2, 3],\n",
       "   'query_type': ['ate',\n",
       "    'det-counterfactual',\n",
       "    'marginal',\n",
       "    'ate',\n",
       "    'det-counterfactual'],\n",
       "   'graph_id': ['mediation', 'diamond', 'arrowhead', 'diamond', 'fork'],\n",
       "   'story_id': ['gender_admission',\n",
       "    'nonsense8',\n",
       "    'smoke_birthWeight',\n",
       "    'vaccine_kills',\n",
       "    'nonsense1'],\n",
       "   'question_property': ['hard',\n",
       "    'nonsense',\n",
       "    'anticommonsense',\n",
       "    'easy',\n",
       "    'nonsense'],\n",
       "   'formal_form': ['E[Y | do(X = 1)] - E[Y | do(X = 0)]',\n",
       "    'Y_{X=1} = 0 | ',\n",
       "    'P(Y)',\n",
       "    'E[Y | do(X = 1)] - E[Y | do(X = 0)]',\n",
       "    'Y_{X=0} = 1 | V2=1']}}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4b494c64-30cf-4017-a5b6-fef823208ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(˵◕ ɛ ◕˵✿) ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀ ❀\n"
     ]
    }
   ],
   "source": [
    "bloomer(n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f191e880-54f1-486d-bd11-3efbbd620ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time Taken: 12.3821 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m q_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m############ INFERENCE ##############\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     17\u001b[0m     response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#####################################\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:819\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 819\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:577\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    566\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    567\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m         position_embeddings,\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:259\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:171\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n\u001b[0;32m--> 171\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m sliding_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_sliding_window\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_window\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_window_layers\n\u001b[1;32m    178\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/cache_utils.py:407\u001b[0m, in \u001b[0;36mDynamicCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m    to the number of layers in the model.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache)\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    409\u001b[0m     key_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    410\u001b[0m     value_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    411\u001b[0m     layer_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    412\u001b[0m     cache_kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    413\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    414\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m        A tuple containing the updated key and value states.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# Update the number of seen tokens\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get 100 cladder questions - \"prompt\" is the question, \"label\" is the answer \n",
    "test = cladder.select(range(10))\n",
    "\n",
    "# time tracking \n",
    "start_time = time.time()\n",
    "\n",
    "# let model \"answer\" 10 questions - unclear how to do eval yet...\n",
    "for i, question in enumerate(test): \n",
    "    \n",
    "    inputs = tokenizer(question['prompt'], return_tensors = 'pt').to(device) \n",
    "    \n",
    "    q_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        ############ INFERENCE ##############\n",
    "        output_ids = model.generate(**inputs, max_length = 256) \n",
    "        response = tokenizer.decode(output_ids[0], skip_special_tokens = True) \n",
    "        #####################################\n",
    "        \n",
    "    q_end_time = time.time() \n",
    "    \n",
    "    # print elapsed time \n",
    "    print(f\"time Taken: {q_end_time - q_start_time:.4f} seconds\\n\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a481d9-f0d1-4783-8dd6-3cce7fe90f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "# Generate response \n",
    "output_ids = model.generate(**inputs, max_length = 256)\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff51b6-5724-4b7b-a547-fd881b4236b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call qwen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "43988071-86b6-411a-a21a-65fb8a3551ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>label</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>rung</th>\n",
       "      <th>query_type</th>\n",
       "      <th>graph_id</th>\n",
       "      <th>story_id</th>\n",
       "      <th>question_property</th>\n",
       "      <th>formal_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>3</td>\n",
       "      <td>nde</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>E[Y_{X=1, V2=0} - Y_{X=0, V2=0}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>no</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>ate</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>E[Y | do(X = 1)] - E[Y | do(X = 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>marginal</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>P(Y)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>no</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>ate</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>E[Y | do(X = 1)] - E[Y | do(X = 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>no</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>3</td>\n",
       "      <td>nie</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>3</td>\n",
       "      <td>nie</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>3</td>\n",
       "      <td>nie</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>no</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>marginal</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>P(Y)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>no</td>\n",
       "      <td>nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan</td>\n",
       "      <td>2</td>\n",
       "      <td>backadj</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>[backdoor adjustment set for Y given X]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>Imagine a self-contained, hypothetical world w...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Let X = husband; V2 = wife; Y = alarm clock.\\n...</td>\n",
       "      <td>3</td>\n",
       "      <td>ett</td>\n",
       "      <td>mediation</td>\n",
       "      <td>alarm</td>\n",
       "      <td>easy</td>\n",
       "      <td>E[Y_{X = 1} - Y_{X = 0} | X = 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt label  \\\n",
       "0   4  Imagine a self-contained, hypothetical world w...   yes   \n",
       "1   7  Imagine a self-contained, hypothetical world w...    no   \n",
       "2   8  Imagine a self-contained, hypothetical world w...   yes   \n",
       "3  15  Imagine a self-contained, hypothetical world w...    no   \n",
       "4  21  Imagine a self-contained, hypothetical world w...    no   \n",
       "5  22  Imagine a self-contained, hypothetical world w...   yes   \n",
       "6  27  Imagine a self-contained, hypothetical world w...   yes   \n",
       "7  28  Imagine a self-contained, hypothetical world w...    no   \n",
       "8  30  Imagine a self-contained, hypothetical world w...    no   \n",
       "9  32  Imagine a self-contained, hypothetical world w...   yes   \n",
       "\n",
       "                                           reasoning  rung query_type  \\\n",
       "0  Let X = husband; V2 = wife; Y = alarm clock.\\n...     3        nde   \n",
       "1  Let X = husband; V2 = wife; Y = alarm clock.\\n...     2        ate   \n",
       "2  Let X = husband; V2 = wife; Y = alarm clock.\\n...     1   marginal   \n",
       "3  Let X = husband; V2 = wife; Y = alarm clock.\\n...     2        ate   \n",
       "4  Let X = husband; V2 = wife; Y = alarm clock.\\n...     3        nie   \n",
       "5  Let X = husband; V2 = wife; Y = alarm clock.\\n...     3        nie   \n",
       "6  Let X = husband; V2 = wife; Y = alarm clock.\\n...     3        nie   \n",
       "7  Let X = husband; V2 = wife; Y = alarm clock.\\n...     1   marginal   \n",
       "8                  nan\\nnan\\nnan\\nnan\\nnan\\nnan\\nnan     2    backadj   \n",
       "9  Let X = husband; V2 = wife; Y = alarm clock.\\n...     3        ett   \n",
       "\n",
       "    graph_id story_id question_property  \\\n",
       "0  mediation    alarm              easy   \n",
       "1  mediation    alarm              easy   \n",
       "2  mediation    alarm              easy   \n",
       "3  mediation    alarm              easy   \n",
       "4  mediation    alarm              easy   \n",
       "5  mediation    alarm              easy   \n",
       "6  mediation    alarm              easy   \n",
       "7  mediation    alarm              easy   \n",
       "8  mediation    alarm              easy   \n",
       "9  mediation    alarm              easy   \n",
       "\n",
       "                               formal_form  \n",
       "0         E[Y_{X=1, V2=0} - Y_{X=0, V2=0}]  \n",
       "1      E[Y | do(X = 1)] - E[Y | do(X = 0)]  \n",
       "2                                     P(Y)  \n",
       "3      E[Y | do(X = 1)] - E[Y | do(X = 0)]  \n",
       "4         E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]  \n",
       "5         E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]  \n",
       "6         E[Y_{X=0, V2=1} - Y_{X=0, V2=0}]  \n",
       "7                                     P(Y)  \n",
       "8  [backdoor adjustment set for Y given X]  \n",
       "9         E[Y_{X = 1} - Y_{X = 0} | X = 1]  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.DataFrame(test)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da28edba-4992-46fe-8506-b140a80a8f50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:819\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 819\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:577\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    566\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    567\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m         position_embeddings,\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:259\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py:171\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n\u001b[0;32m--> 171\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m sliding_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_sliding_window\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_window\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_window_layers\n\u001b[1;32m    178\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/cache_utils.py:449\u001b[0m, in \u001b[0;36mDynamicCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx] \u001b[38;5;241m=\u001b[39m value_states\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx], value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c7b5fd7-4b19-49b3-bdd8-fd72076f644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 μs, sys: 2 μs, total: 7 μs\n",
      "Wall time: 16 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Hello, I/'d like to plan a birthday for my dog, can you help me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67571c8e-8a1f-45a9-9a1c-37b676ea1337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I/'d like to plan a birthday for my dog, can you help me? I have a 2-year-old Golden Retriever, named Max. I want to know if I can do a dog birthday party, and if so, what are the steps I should follow. Also, I'm not sure about the details of the dog's birthday, so maybe I can just find out when that is. Additionally, I'm not sure how to organize this event. I need to find a way to make it fun and engaging for my dog. Any help would be greatly appreciated.\n",
      "Alright, so I'm trying to plan a birthday for my golden retriever named Max. He's just two years old, so I need to figure out how to make that special day even more memorable. I've heard that birthdays for dogs can be a bit different from ours because dogs aren't humans, but I'm not entirely sure how that works. I also need to find out when Max's birthday is so I can plan accordingly. Let me break this down step by step.\n",
      "\n",
      "First, I should figure out what Max's birthday is. I know that humans have birthdays every day, but dogs have their own unique way of celebrating. I think it's called a dog\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
