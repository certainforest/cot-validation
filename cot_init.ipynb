{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2c683da2-5fca-45ed-915c-add0bb5b6ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from src.utils import batch_generate, tokens_generate, run_inference\n",
    "from src.mem import check_memory\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import huggingface\n",
    "from datetime import datetime\n",
    "load_dotenv('secrets.env')\n",
    "\n",
    "with open(\"config/config.yaml\", \"r\") as f: \n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model_str = 'deepseek_r1_qwendistill_1.5'\n",
    "eval_df = 'big_bench'\n",
    "\n",
    "ds = config['datasets'][eval_df]\n",
    "model_path = config['models'][model_str]['path']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e3e26-b92a-4065-8a3c-68f7af9d6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f71afb9-81d8-4582-94cf-d861c80cefe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(ds['source'], ds['subset']).shuffle(config['seed'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path) \n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea557dc-0b4c-4c15-a0ec-735d5fdc4d39",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "Goal: test model performance on 100 BBH questions w/ chain of thought reasoning =D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fec77470-8343-4a62-b7de-fcbb9685f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset['train'][0:10])\n",
    "df['input'] = apply_instruct_format(df['input'], model = model_str, is_math = True)\n",
    "\n",
    "batches = batch_generate(df, ds['input_column'], ds['target_column'])\n",
    "tokens = tokens_generate(batches, tokenizer, device = 'mps')\n",
    "res = run_inference(model, tokens, tokenizer, time_tracking = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bf91568f-01d4-41dc-9651-d3fb9375f402",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out whether Kate didn't put oil in the machine causing it\n",
      "<think>\n",
      "Okay, so I need to figure out whether Alex caused the plants to dry out. Let me break\n",
      "<think>\n",
      "Okay, so I'm trying to figure out whether the motorboat started because Ned changed the motor's\n",
      "<think>\n",
      "Okay, so I'm trying to figure out whether the drunk driver caused the injury to Joe's son\n",
      "<think>\n",
      "Okay, so I need to figure out if Frank T. was intentionally shooting his neighbor in the body\n",
      "<think>\n",
      "Okay, so I'm trying to figure out whether Brown intentionally rolled a six on a die to deton\n",
      "<think>\n",
      "Okay, so I need to figure out if Susan intentionally increased the prominence of the Atlantic division. Let\n",
      "<think>\n",
      "Okay, so I'm trying to figure out whether the department budget committee causes the approval of Prof.\n",
      "<think>\n",
      "Okay, so I'm trying to figure out whether Louie won the $100 bet because\n",
      "<think>\n",
      "Okay, so I'm trying to figure out how a typical person would answer this question about causation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for result in res: \n",
    "    for single_response in result['response']:\n",
    "        cleaned = re.sub(r'^.*?<think>', '<think>', single_response, flags=re.DOTALL)\n",
    "        print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f53e623f-5e82-495b-8487-52235696a17b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2e891509-1a56-4b83-890d-b7c5636a97a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[193], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m res]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# [print(result['response'].replace(result['input'], '').strip()) for result in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "77b19ee0-6baa-4cea-9442-8ba43b9fe4fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['max_tokens'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(test[\u001b[38;5;241m3\u001b[39m], padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, truncation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m full_response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(response[\u001b[38;5;241m0\u001b[39m], skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2012\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2009\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   2011\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2012\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mats/cot-validation/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:1388\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1389\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1391\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['max_tokens'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = tokenizer(test[3], padding = True, truncation = True, return_tensors = 'pt').to(device)\n",
    "response = model.generate(**tokens, temperature = 0.6, max_tokens = 2000)\n",
    "full_response = tokenizer.decode(response[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c83311d4-c838-4d9d-beca-42a63370c502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I'm trying to figure out whether the drunk driver caused the injury to Joe's son\""
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_response.replace(test[3], '').strip() # nice! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f5044-d3c7-4513-94a7-5d1c26ac6043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
