seed: 88

models:
    deepseek_r1_qwendistill_1.5: 
        path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
        format: 
            template: |-
                <｜begin▁of▁sentence｜><｜User｜>
                {answer_format} {content}<｜Assistant｜>{think_tag}
    deepseek_r1_qwendistill_7: 
        path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        format: 
            template: |-
                <｜begin▁of▁sentence｜><｜User｜>
                {answer_format} {content}<｜Assistant｜>{think_tag}
    deepseek_r1_qwendistill_14: 
        path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
        format: 
            template: |-
                <｜begin▁of▁sentence｜><｜User｜>
                {answer_format} {content}<｜Assistant｜>{think_tag}
    deepseek_v3: 
        path: "deepseek-ai/DeepSeek-V3"
        format: 
            template: |-
                <｜begin▁of▁sentence｜><｜User｜>
                {answer_format} {content}<｜Assistant｜>

datasets:
    cladder:
        source: "causalNLP/cladder"
        subset: "full_v1.5_default"
        input_column: "prompt"
        target_column: "label"
    big_bench: 
        source: "maveriq/bigbenchhard"
        subset: "causal_judgement"
        input_column: "input" 
        target_column: "target"
        additional_options:
            question_types: ['boolean_expressions', 'date_understanding', 'disambiguation_qa', 'dyck_languages', 'formal_fallacies', 'geometric_shapes', 'hyperbaton', 
            'logical_deduction_five_objects', 'logical_deduction_seven_objects', 'logical_deduction_three_objects', 'movie_recommendation', 'multistep_arithmetic_two', 'navigate', 'object_counting', 
            'penguins_in_a_table', 'reasoning_about_colored_objects', 'ruin_names', 'salient_translation_error_detection', 'snarks', 'sports_understanding', 'temporal_sequences', 
            'tracking_shuffled_objects_five_objects', 'tracking_shuffled_objects_seven_objects', 'tracking_shuffled_objects_three_objects', 'web_of_lies', 'word_sorting']
